{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'project/'\n",
      "/home/rapids/notebooks/project\n"
     ]
    }
   ],
   "source": [
    "%cd project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 12) # (w, h)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import sample\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1380 346 1380\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dir = \"./matrices/adj/dia/\"\n",
    "arrays = []\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        arrays.append(np.load(dir + \"/\"+filename))\n",
    "\n",
    "v = arrays\n",
    "img_size = v[0].shape[1]\n",
    "v_train, v_test = train_test_split(v, test_size=0.2, random_state=42)\n",
    "v = v_train\n",
    "print(len(v_train), len(v_test), len(v))\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 6000\n",
    "batch_size = 345\n",
    "# batch_size = 146\n",
    "# n_epochs = 3000\n",
    "# batch_size = 128\n",
    "\n",
    "lr = 0.002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "# n_cpu = 8\n",
    "latent_dim = 100\n",
    "channels = 1\n",
    "# sample_interval = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_data, transform=None):\n",
    "        super(VectorialDataset, self).__init__()\n",
    "        self.input_data = torch.tensor(np.expand_dims(input_data, axis = 1)).float()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = self.input_data[idx, :]\n",
    "\n",
    "        if self.transform:\n",
    "          sample = self.transform(sample)\n",
    "\n",
    "        return sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([345, 1, 64, 64])\n",
      "1 torch.Size([345, 1, 64, 64])\n",
      "2 torch.Size([345, 1, 64, 64])\n",
      "3 torch.Size([345, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "training_set = VectorialDataset(input_data=v)\n",
    "dataloader = torch.utils.data.DataLoader(training_set, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "for i, imgs in enumerate(dataloader):\n",
    "  print(i, imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 8\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 512 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),  # De init_size a init_size * 2\n",
    "            nn.Conv2d(512, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),  # Añadimos dropout\n",
    "            nn.Upsample(scale_factor=2),  # De init_size * 2 a init_size * 4\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),  # Añadimos dropout\n",
    "            nn.Upsample(scale_factor=2),  # De init_size * 4 a init_size * 8 (img_size)\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "            nn.ReLU()  # Cambiado de Tanh a Sigmoid  # Usamos Tanh para salida entre [-1, 1]\n",
    "            # También puedes probar con nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 512, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [\n",
    "                nn.utils.spectral_norm(nn.Conv2d(in_filters, out_filters, 3, 2, 1)),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.3)  # Reducimos ligeramente el dropout\n",
    "            ]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # Tamaño después del downsampling\n",
    "        ds_size = img_size // 2 ** 4  # 4 capas con stride 2\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        #print(\"imgshape\" + str(img.shape))\n",
    "        #print(\"outshape\" + str(out.shape))\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).to(device)\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones(real_samples.size(0), 1).to(device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "# generator = Generator()\n",
    "# discriminator = Discriminator()\n",
    "\n",
    "# if cuda:\n",
    "#     generator.cuda()\n",
    "#     discriminator.cuda()\n",
    "#     # adversarial_loss.cuda()\n",
    "\n",
    "# # Initialize weights\n",
    "# generator.apply(weights_init_normal)\n",
    "# discriminator.apply(weights_init_normal)\n",
    "\n",
    "\n",
    "# # Optimizers\n",
    "# optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr*10, betas=(b1, b2),weight_decay=1e-4)\n",
    "# optimizer_D = torch.optim.Adam(discriminator.parameters(), lr= lr, betas=( b1,  b2), weight_decay=1e-4)\n",
    "\n",
    "# Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_losses = []\n",
    "# D_losses = []\n",
    "\n",
    "# real_scores = np.zeros(n_epochs)\n",
    "# fake_scores = np.zeros(n_epochs)\n",
    "\n",
    "# lambda_gp = 10  # Coeficiente de penalización de gradiente\n",
    "# n_critic = 5    # Número de pasos de entrenamiento del discriminador por cada paso del generador\n",
    "\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     for i, imgs in enumerate(dataloader):\n",
    "\n",
    "#         # Adversarial ground truths vectors\n",
    "#         valid = Variable(Tensor(imgs.shape[0], 1).fill_(0.9), requires_grad=False)\n",
    "#         fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.1), requires_grad=False)\n",
    "\n",
    "#         # Configure input\n",
    "#         real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "#         real_imgs = real_imgs + 0.05 * torch.randn_like(real_imgs)  # Añadir ruido pequeño a las imágenes reales\n",
    "\n",
    "\n",
    "#         # -----------------\n",
    "#         #  Train Generator\n",
    "#         # -----------------\n",
    "\n",
    "#         optimizer_G.zero_grad()\n",
    "\n",
    "#         # Sample noise as generator input\n",
    "#         z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0],  latent_dim))))\n",
    "\n",
    "#         # Generate a batch of images\n",
    "#         gen_imgs = generator(z) # shape '[146, 128, 16, 16]' is invalid for input of size 19136512\n",
    "\n",
    "#         # Loss measures generator's ability to fool the discriminator\n",
    "#         g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "#         g_loss.backward()\n",
    "#         optimizer_G.step()\n",
    "\n",
    "#         # ---------------------\n",
    "#         #  Train Discriminator\n",
    "#         # ---------------------\n",
    "\n",
    "#         optimizer_D.zero_grad()\n",
    "\n",
    "#         # Measure discriminator's ability to classify real from generated samples\n",
    "#         real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "#         fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "#         d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "#         #Accuracies\n",
    "#         outputs = discriminator(real_imgs)\n",
    "#         real_score = outputs\n",
    "#         outputs = discriminator(gen_imgs.detach())\n",
    "#         fake_score = outputs\n",
    "\n",
    "#         d_loss.backward()\n",
    "#         optimizer_D.step()\n",
    "\n",
    "#         if epoch%10 == 0 and i == len(dataloader)-1:\n",
    "#             print(\n",
    "#             \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "#             % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "#             )\n",
    "                    \n",
    "#         # Save Losses for plotting later and accuracies\n",
    "#         G_losses.append(g_loss.item())\n",
    "#         D_losses.append(d_loss.item())\n",
    "\n",
    "#         real_scores[epoch] = real_scores[epoch]*(i/(i+1.)) + real_score.mean().data*(1./(i+1.))\n",
    "#         fake_scores[epoch] = fake_scores[epoch]*(i/(i+1.)) + fake_score.mean().data*(1./(i+1.))\n",
    "\n",
    "# print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "# from scipy.stats import entropy\n",
    "\n",
    "# def calculate_score(generator, real_adj_matrices, num_samples=100):\n",
    "#     \"\"\"\n",
    "#     Calcula un score basado en la similitud de la distribución de grados\n",
    "#     entre las redes reales y las redes generadas.\n",
    "\n",
    "#     Args:\n",
    "#         generator: El modelo generador entrenado.\n",
    "#         real_adj_matrices: Lista o arreglo de matrices de adyacencia reales.\n",
    "#         num_samples: Número de redes a generar para la evaluación.\n",
    "\n",
    "#     Returns:\n",
    "#         score: Un valor que indica la similitud entre las redes reales y generadas.\n",
    "#     \"\"\"\n",
    "#     # Obtener el grado máximo de las redes reales\n",
    "#     max_degree_real = get_max_degree(real_adj_matrices)\n",
    "    \n",
    "    \n",
    "#     # Generar redes sintéticas\n",
    "#     generated_adj_matrices = []\n",
    "#     generator.eval()  # Poner el generador en modo evaluación\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(num_samples):\n",
    "#             z = torch.randn(1, 100).to(device)\n",
    "#             gen_adj = generator(z).cpu().numpy()\n",
    "#             gen_adj = gen_adj.squeeze()\n",
    "#             # Si usaste Tanh en la salida, desnormaliza\n",
    "#             # gen_adj = (gen_adj + 1) / 2  # Rango [0, 1]\n",
    "#             # Binarizar la matriz de adyacencia\n",
    "#             gen_adj_bin = (gen_adj > 0.5).astype(int)\n",
    "#             generated_adj_matrices.append(gen_adj_bin)\n",
    "#     # Obtener el grado máximo de las redes generadas\n",
    "#     max_degree_generated = get_max_degree(generated_adj_matrices)\n",
    "    \n",
    "#     # Obtener el grado máximo común\n",
    "#     max_degree = max(max_degree_real, max_degree_generated)\n",
    "    \n",
    "#     # Obtener distribuciones de grados con el mismo tamaño\n",
    "#     degree_distribution_real = get_average_degree_distribution(real_adj_matrices, max_degree)\n",
    "    \n",
    "#     # Obtener distribución de grados de las redes generadas\n",
    "#     degree_distribution_generated = get_average_degree_distribution(generated_adj_matrices, max_degree)\n",
    "    \n",
    "\n",
    "#     # Calcular la distancia entre las distribuciones de grados\n",
    "#     # Usamos la distancia de Jensen-Shannon\n",
    "#     score = jensen_shannon_distance(degree_distribution_real, degree_distribution_generated)\n",
    "\n",
    "#     # Volver el generador a modo de entrenamiento\n",
    "#     generator.train()\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# def get_max_degree(adj_matrices):\n",
    "#     \"\"\"\n",
    "#     Obtiene el grado máximo de una lista de matrices de adyacencia.\n",
    "#     \"\"\"\n",
    "#     max_degree = 0\n",
    "#     for adj in adj_matrices:\n",
    "#         G = nx.from_numpy_array(adj)\n",
    "#         degrees = [degree for node, degree in G.degree()]\n",
    "#         max_degree = max(max_degree, max(degrees))\n",
    "#     return max_degree\n",
    "\n",
    "\n",
    "# def get_average_degree_distribution(adj_matrices, max_degree):\n",
    "#     \"\"\"\n",
    "#     Calcula la distribución promedio de grados a partir de una lista de matrices de adyacencia.\n",
    "\n",
    "#     Args:\n",
    "#         adj_matrices: Lista o arreglo de matrices de adyacencia.\n",
    "\n",
    "#     Returns:\n",
    "#         degree_distribution: Distribución de grados normalizada.\n",
    "#     \"\"\"\n",
    "#     degree_hist = np.zeros(max_degree + 1)\n",
    "#     total_nodes = 0\n",
    "#     for adj in adj_matrices:\n",
    "#         G = nx.from_numpy_array(adj)\n",
    "#         degrees = [degree for node, degree in G.degree()]\n",
    "#         total_nodes += len(degrees)\n",
    "#         for degree in degrees:\n",
    "#             degree_hist[degree] += 1\n",
    "#     # Normalizar la distribución\n",
    "#     degree_distribution = degree_hist / total_nodes\n",
    "#     return degree_distribution\n",
    "\n",
    "# def jensen_shannon_distance(p, q):\n",
    "#     \"\"\"\n",
    "#     Calcula la distancia de Jensen-Shannon entre dos distribuciones de probabilidad.\n",
    "\n",
    "#     Args:\n",
    "#         p: Distribución de probabilidad 1.\n",
    "#         q: Distribución de probabilidad 2.\n",
    "\n",
    "#     Returns:\n",
    "#         js_distance: Distancia de Jensen-Shannon.\n",
    "#     \"\"\"\n",
    "#     # Asegurarse de que p y q son arrays numpy y normalizados\n",
    "#     p = np.array(p)\n",
    "#     q = np.array(q)\n",
    "#     p = p / np.sum(p)\n",
    "#     q = q / np.sum(q)\n",
    "\n",
    "#     m = 0.5 * (p + q)\n",
    "#     js_distance = 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "#     return js_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_score(generator, real_adj_matrices, num_samples=100):\n",
    "    \"\"\"\n",
    "    Calcula un score basado en la similitud de la distribución de grados\n",
    "    entre las redes reales y las redes generadas.\n",
    "\n",
    "    Args:\n",
    "        generator: El modelo generador entrenado.\n",
    "        real_adj_matrices: Lista o arreglo de matrices de adyacencia reales.\n",
    "        num_samples: Número de redes a generar para la evaluación.\n",
    "\n",
    "    Returns:\n",
    "        score: Un valor que indica la similitud entre las redes reales y generadas.\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generar redes sintéticas\n",
    "        generated_adj_matrices = []\n",
    "        for _ in range(num_samples):\n",
    "            z = torch.randn(1, 100, device=device)\n",
    "            gen_adj = generator(z)\n",
    "            gen_adj = gen_adj.squeeze()\n",
    "            # Si usaste Tanh en la salida, desnormaliza\n",
    "            gen_adj = (gen_adj + 1) / 2  # Rango [0, 1]\n",
    "            # Binarizar la matriz de adyacencia\n",
    "            gen_adj_bin = (gen_adj > 0.5).float()\n",
    "            generated_adj_matrices.append(gen_adj_bin)\n",
    "    \n",
    "    # Convertir lista a tensor\n",
    "    generated_adj_tensor = torch.stack(generated_adj_matrices)  # Shape: (num_samples, N, N)\n",
    "\n",
    "    # Calcular distribución de grados para las redes generadas\n",
    "    degree_distribution_generated = get_degree_distribution_tensor(generated_adj_tensor)\n",
    "\n",
    "    # Procesar las redes reales\n",
    "    real_adj_tensors = [torch.tensor(adj, dtype=torch.float32, device=device) for adj in real_adj_matrices]\n",
    "    real_adj_tensor = torch.stack(real_adj_tensors)  # Shape: (num_samples, N, N)\n",
    "\n",
    "    # Calcular distribución de grados para las redes reales\n",
    "    degree_distribution_real = get_degree_distribution_tensor(real_adj_tensor)\n",
    "\n",
    "    # Asegurar que las distribuciones tengan la misma longitud\n",
    "    max_length = max(len(degree_distribution_real), len(degree_distribution_generated))\n",
    "    degree_distribution_real = pad_distribution(degree_distribution_real, max_length)\n",
    "    degree_distribution_generated = pad_distribution(degree_distribution_generated, max_length)\n",
    "\n",
    "    # Calcular la distancia de Jensen-Shannon en GPU\n",
    "    score = jensen_shannon_distance_torch(degree_distribution_real, degree_distribution_generated)\n",
    "\n",
    "    generator.train()\n",
    "\n",
    "    return score.item()\n",
    "\n",
    "def get_max_degree(adj_matrices):\n",
    "    \"\"\"\n",
    "    Obtiene el grado máximo de una lista de matrices de adyacencia.\n",
    "    \"\"\"\n",
    "    max_degree = 0\n",
    "    for adj in adj_matrices:\n",
    "        G = nx.from_numpy_array(adj)\n",
    "        degrees = [degree for node, degree in G.degree()]\n",
    "        max_degree = max(max_degree, max(degrees))\n",
    "    return max_degree\n",
    "\n",
    "def get_degree_distribution_tensor(adj_tensor):\n",
    "    \"\"\"\n",
    "    Calcula la distribución promedio de grados a partir de un tensor de matrices de adyacencia.\n",
    "    \"\"\"\n",
    "    # adj_tensor tiene forma (num_samples, N, N)\n",
    "    degrees = adj_tensor.sum(dim=2)  # Sumar sobre las columnas para obtener grados de filas\n",
    "    degrees = degrees.view(-1).long()  # Aplanar y convertir a enteros\n",
    "    max_degree = degrees.max().item()\n",
    "    degree_hist = torch.bincount(degrees, minlength=max_degree+1).float()\n",
    "    # Normalizar la distribución\n",
    "    degree_distribution = degree_hist / degrees.numel()\n",
    "    return degree_distribution\n",
    "\n",
    "def get_average_degree_distribution(adj_matrices, max_degree):\n",
    "    \"\"\"\n",
    "    Calcula la distribución promedio de grados a partir de una lista de matrices de adyacencia.\n",
    "\n",
    "    Args:\n",
    "        adj_matrices: Lista o arreglo de matrices de adyacencia.\n",
    "\n",
    "    Returns:\n",
    "        degree_distribution: Distribución de grados normalizada.\n",
    "    \"\"\"\n",
    "    degree_hist = np.zeros(max_degree + 1)\n",
    "    total_nodes = 0\n",
    "    for adj in adj_matrices:\n",
    "        G = nx.from_numpy_array(adj)\n",
    "        degrees = [degree for node, degree in G.degree()]\n",
    "        total_nodes += len(degrees)\n",
    "        for degree in degrees:\n",
    "            degree_hist[degree] += 1\n",
    "    # Normalizar la distribución\n",
    "    degree_distribution = degree_hist / total_nodes\n",
    "    return degree_distribution\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    \"\"\"\n",
    "    Calcula la distancia de Jensen-Shannon entre dos distribuciones de probabilidad.\n",
    "\n",
    "    Args:\n",
    "        p: Distribución de probabilidad 1.\n",
    "        q: Distribución de probabilidad 2.\n",
    "\n",
    "    Returns:\n",
    "        js_distance: Distancia de Jensen-Shannon.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que p y q son arrays numpy y normalizados\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "\n",
    "    m = 0.5 * (p + q)\n",
    "    js_distance = 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "    return js_distance\n",
    "\n",
    "def pad_distribution(distribution, max_length):\n",
    "    \"\"\"\n",
    "    Asegura que la distribución tenga una longitud de max_length rellenando con ceros si es necesario.\n",
    "    \"\"\"\n",
    "    if len(distribution) < max_length:\n",
    "        padding = torch.zeros(max_length - len(distribution), device=distribution.device)\n",
    "        distribution = torch.cat([distribution, padding])\n",
    "    return distribution\n",
    "\n",
    "def jensen_shannon_distance_torch(p, q):\n",
    "    \"\"\"\n",
    "    Calcula la distancia de Jensen-Shannon entre dos distribuciones de probabilidad usando PyTorch.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que p y q sumen 1\n",
    "    p = p / p.sum()\n",
    "    q = q / q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    # Añadir una pequeña constante para evitar log(0)\n",
    "    eps = 1e-8\n",
    "    p = p + eps\n",
    "    q = q + eps\n",
    "    m = m + eps\n",
    "    jsd = 0.5 * (p * (p / m).log()).sum() + 0.5 * (q * (q / m).log()).sum()\n",
    "    return jsd.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'[' was never closed (1552287376.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    lr_combs = [(0.002, 0.0002), (0.0002, 0.00002),\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '[' was never closed\n"
     ]
    }
   ],
   "source": [
    "# Inicialización de listas para almacenar las pérdidas\n",
    "'''\n",
    "scores = []\n",
    "\n",
    "lambda_gp = 10  # Coeficiente de penalización de gradiente\n",
    "n_critic = 5    # Número de pasos de entrenamiento del discriminador por cada paso del generador\n",
    "\n",
    "# for lgp in [1,5,10,15,20]:\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    # adversarial_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2),weight_decay=1e-4)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr= lr, betas=( b1,  b2), weight_decay=1e-4)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "losses_G = []\n",
    "losses_D = []\n",
    "critic_losses = []\n",
    "validity_real = []\n",
    "validity_fake = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "\n",
    "        # Configuración de tensores\n",
    "        real_imgs = imgs.to(device)\n",
    "        # real_imgs = Variable(imgs.type(Tensor))\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Generación de ruido y muestras falsas\n",
    "        z = torch.randn(imgs.size(0), 100).to(device)\n",
    "        fake_imgs = generator(z).detach()\n",
    "        fake_imgs.requires_grad = True\n",
    "\n",
    "        # Pérdida del discriminador\n",
    "        real_validity = discriminator(real_imgs)\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "        validity_real.append(real_validity.mean().item())\n",
    "        validity_fake.append(fake_validity.mean().item())\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "        critic_loss = -torch.mean(real_validity) + torch.mean(fake_validity)\n",
    "        critic_losses.append(critic_loss.item())\n",
    "        loss_D = critic_loss + lambda_gp * gradient_penalty\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Entrenamiento del generador cada n_critic iteraciones\n",
    "        if i % n_critic == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generación de muestras y cálculo de pérdida\n",
    "            gen_imgs = generator(z)\n",
    "            gen_validity = discriminator(gen_imgs)\n",
    "            loss_G = -torch.mean(gen_validity)\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "        \n",
    "        \n",
    "        # Almacenar las pérdidas al final de cada época\n",
    "        losses_G.append(loss_G.item())\n",
    "        losses_D.append(loss_D.item())\n",
    "\n",
    "    # # Mostrar las pérdidas cada 10 épocas\n",
    "    # if epoch % 10 == 0:\n",
    "    #     # print(f\"Epoch [{epoch}/{n_epochs}] | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f} | Score: {score:.4f}\")\n",
    "    #     print(f\"Epoch [{epoch}/{n_epochs}] | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f}\")\n",
    "    if epoch % 10 == 0:\n",
    "        score = calculate_score(generator, v_train, num_samples=100)\n",
    "        scores.append(score)\n",
    "        print(f\"Epoch [{epoch}/{n_epochs}] | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f} | Score: {score:.4f} | Critic Loss: {critic_loss.item():.4f}\")\n",
    "        # Guardar modelos\n",
    "        # torch.save(generator.state_dict(), f\"generator_epoch_{epoch}.pth\")\n",
    "        # torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch}.pth\")\n",
    "    \n",
    "\n",
    "# score = calculate_score(generator, v_train, num_samples=100)\n",
    "# scores.append(score)\n",
    "print(f\"Score: {score:.4f}\")\n",
    "print(\"end\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_plots(losses_D, losses_G, scores, critic_losses):\n",
    "  #Plot Losses\n",
    "  plt.title(\"MoGAN-Training\", size = 23, fontweight=\"bold\")\n",
    "  plt.plot(losses_D,label=\"Discriminator\")\n",
    "  plt.plot(losses_G,label=\"Generator\")\n",
    "  plt.xlabel(\"iterations\", size = 23)\n",
    "  plt.ylabel(\"loss\", size = 23)\n",
    "  plt.tick_params(labelsize=20)\n",
    "  plt.legend(prop={'size': 21})\n",
    "  # plt.savefig(\"./lossMoGAN_sem.pdf\")  \n",
    "  plt.show(block=False)\n",
    "\n",
    "# Plot Scores\n",
    "# plt.title(\"MoGAN: Validation values\", size = 23, fontweight=\"bold\")\n",
    "# plt.plot(validity_fake, label='synthetic score')\n",
    "# plt.plot(validity_real, label='real score')    \n",
    "# plt.xlabel(\"epochs\", size = 23)\n",
    "# plt.ylabel(\"score\", size = 23)\n",
    "# plt.tick_params(labelsize=20)\n",
    "# plt.legend(prop={'size': 18})\n",
    "# # plt.savefig(\"./scoresMoGAN_sem.pdf\")\n",
    "# plt.show(block=False)\n",
    "\n",
    "  # Plot Scores\n",
    "  plt.title(\"MoGAN: Scores\", size = 23, fontweight=\"bold\")\n",
    "  plt.plot(scores, label='synthetic score')\n",
    "  # plt.plot(real_scores, label='real score')    \n",
    "  plt.xlabel(\"epochs\", size = 23)\n",
    "  plt.ylabel(\"score\", size = 23)\n",
    "  plt.tick_params(labelsize=20)\n",
    "  plt.legend(prop={'size': 18})\n",
    "  # plt.savefig(\"./scoresMoGAN_sem.pdf\")\n",
    "  plt.show(block=False)\n",
    "\n",
    "  # Plot Scores\n",
    "  plt.title(\"MoGAN: Critic Loss\", size = 23, fontweight=\"bold\")\n",
    "  plt.plot(critic_losses, label='critic loss')\n",
    "  # plt.plot(real_scores, label='real score')    \n",
    "  plt.xlabel(\"epochs\", size = 23)\n",
    "  plt.ylabel(\"score\", size = 23)\n",
    "  plt.tick_params(labelsize=20)\n",
    "  plt.legend(prop={'size': 18})\n",
    "  # plt.savefig(\"./scoresMoGAN_sem.pdf\")\n",
    "  plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake set and dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(generator, discriminator, sufix):\n",
    "  torch.save(generator.state_dict(), \"./models/generator_\"+sufix+\".pth\")\n",
    "  torch.save(discriminator.state_dict(), \"./models/discriminator_\"+sufix+\".pth\")\n",
    "\n",
    "def load_models(generator, discriminator, sufix):\n",
    "  generator.load_state_dict(torch.load(\"./models/generator_\"+sufix+\".pth\"))\n",
    "  discriminator.load_state_dict(torch.load(\"./models/discriminator_\"+sufix+\".pth\"))\n",
    "  return generator, discriminator\n",
    "  \n",
    "def save_tests(generator, name_sufix):\n",
    "  fake_set = []\n",
    "  t = torch.tensor(np.random.normal(0, 1, (len(v_test),  latent_dim)), device='cuda') #instead of batch_size should be len(v_test), but depends on your GPU power.\n",
    "  t = generator(t).detach().cpu().numpy()\n",
    "  print(t.shape)\n",
    "\n",
    "  for i in range(0, t.shape[0]):\n",
    "    fake_set.append(np.rint(t[i][0]).astype(int))\n",
    "\n",
    "  print(\"len of fake set\", len(fake_set))\n",
    "\n",
    "\n",
    "  with open( \"./generations/fake_set_\"+name_sufix+\".txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(fake_set, fp) \n",
    "\n",
    "  # with open(\"./generations/v_test_\"+name_sufix+\".txt\", \"wb\") as fp:   #Pickling\n",
    "  #   pickle.dump(v_test, fp) \n",
    "\n",
    "  # with open(\"./generations/v_train_\"+name_sufix+\".txt\", \"wb\") as fp:   #Pickling\n",
    "  #   pickle.dump(v_train, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fake_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfake_set\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fake_set' is not defined"
     ]
    }
   ],
   "source": [
    "fake_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/6000] | Loss D: 8.2076 | Loss G: -0.0117 | Score: 0.5382 | Critic Loss: -0.7858\n",
      "Epoch [10/6000] | Loss D: 1.2105 | Loss G: -3.1148 | Score: 0.7023 | Critic Loss: -6.6878\n",
      "Epoch [20/6000] | Loss D: -2.8343 | Loss G: -5.0660 | Score: 0.7181 | Critic Loss: -8.9433\n",
      "Epoch [30/6000] | Loss D: -0.1130 | Loss G: -7.7846 | Score: 0.7267 | Critic Loss: -5.4978\n",
      "Epoch [40/6000] | Loss D: 0.5029 | Loss G: -6.5495 | Score: 0.7386 | Critic Loss: -4.4489\n",
      "Epoch [50/6000] | Loss D: -3.3513 | Loss G: -4.9601 | Score: 0.7244 | Critic Loss: -6.7190\n",
      "Epoch [60/6000] | Loss D: -5.0450 | Loss G: -8.5392 | Score: 0.7078 | Critic Loss: -8.9007\n",
      "Epoch [70/6000] | Loss D: -5.1357 | Loss G: -12.4145 | Score: 0.7107 | Critic Loss: -7.3899\n",
      "Epoch [80/6000] | Loss D: -9.0136 | Loss G: -9.5780 | Score: 0.7156 | Critic Loss: -10.4103\n",
      "Epoch [90/6000] | Loss D: -10.5797 | Loss G: -5.3389 | Score: 0.7115 | Critic Loss: -11.7518\n",
      "Epoch [100/6000] | Loss D: -11.6465 | Loss G: -9.7953 | Score: 0.7073 | Critic Loss: -12.3441\n",
      "Epoch [110/6000] | Loss D: -14.4574 | Loss G: -5.7613 | Score: 0.6978 | Critic Loss: -15.0100\n",
      "Epoch [120/6000] | Loss D: -9.3003 | Loss G: -9.4019 | Score: 0.7024 | Critic Loss: -9.7972\n",
      "Epoch [130/6000] | Loss D: -7.8970 | Loss G: -7.2164 | Score: 0.6840 | Critic Loss: -8.3422\n",
      "Epoch [140/6000] | Loss D: -10.1648 | Loss G: -11.5926 | Score: 0.6854 | Critic Loss: -10.6688\n",
      "Epoch [150/6000] | Loss D: -9.0753 | Loss G: -3.6685 | Score: 0.6670 | Critic Loss: -9.4901\n",
      "Epoch [160/6000] | Loss D: -5.6751 | Loss G: -6.2125 | Score: 0.6592 | Critic Loss: -5.9609\n",
      "Epoch [170/6000] | Loss D: -1.3371 | Loss G: 3.3860 | Score: 0.6599 | Critic Loss: -1.7688\n",
      "Epoch [180/6000] | Loss D: -5.3532 | Loss G: 2.9436 | Score: 0.6329 | Critic Loss: -5.8201\n",
      "Epoch [190/6000] | Loss D: -2.8408 | Loss G: -4.2104 | Score: 0.6574 | Critic Loss: -3.1713\n",
      "Epoch [200/6000] | Loss D: -6.1435 | Loss G: -4.5431 | Score: 0.6123 | Critic Loss: -6.5287\n",
      "Epoch [210/6000] | Loss D: -5.5424 | Loss G: -0.1594 | Score: 0.6147 | Critic Loss: -5.9347\n",
      "Epoch [220/6000] | Loss D: -1.7894 | Loss G: -8.9615 | Score: 0.5969 | Critic Loss: -2.1848\n",
      "Epoch [230/6000] | Loss D: -2.0482 | Loss G: -7.7212 | Score: 0.6172 | Critic Loss: -2.4868\n",
      "Epoch [240/6000] | Loss D: -4.9722 | Loss G: -8.0769 | Score: 0.5969 | Critic Loss: -5.3128\n",
      "Epoch [250/6000] | Loss D: -4.5315 | Loss G: -13.2993 | Score: 0.5806 | Critic Loss: -4.9234\n",
      "Epoch [260/6000] | Loss D: -4.1614 | Loss G: 1.2813 | Score: 0.6017 | Critic Loss: -4.5932\n",
      "Epoch [270/6000] | Loss D: -5.6450 | Loss G: -7.9599 | Score: 0.5851 | Critic Loss: -6.0267\n",
      "Epoch [280/6000] | Loss D: -11.2778 | Loss G: -15.2701 | Score: 0.5879 | Critic Loss: -11.5945\n",
      "Epoch [290/6000] | Loss D: -11.0591 | Loss G: -0.7120 | Score: 0.5734 | Critic Loss: -11.6576\n",
      "Epoch [300/6000] | Loss D: -8.7995 | Loss G: -4.0158 | Score: 0.5782 | Critic Loss: -9.4008\n",
      "Epoch [310/6000] | Loss D: -11.7058 | Loss G: 1.2667 | Score: 0.5806 | Critic Loss: -12.6019\n",
      "Epoch [320/6000] | Loss D: -2.9986 | Loss G: 3.8065 | Score: 0.5797 | Critic Loss: -3.8202\n",
      "Epoch [330/6000] | Loss D: -12.5419 | Loss G: 10.8947 | Score: 0.5841 | Critic Loss: -13.4970\n",
      "Epoch [340/6000] | Loss D: -12.6058 | Loss G: -7.3725 | Score: 0.5687 | Critic Loss: -13.2987\n",
      "Epoch [350/6000] | Loss D: -13.2094 | Loss G: -10.3702 | Score: 0.5763 | Critic Loss: -14.0071\n",
      "Epoch [360/6000] | Loss D: -6.2981 | Loss G: -29.3613 | Score: 0.5764 | Critic Loss: -6.5943\n",
      "Epoch [370/6000] | Loss D: -9.7803 | Loss G: -5.7762 | Score: 0.5823 | Critic Loss: -10.5826\n",
      "Epoch [380/6000] | Loss D: -6.2899 | Loss G: -11.1526 | Score: 0.5530 | Critic Loss: -6.7061\n",
      "Epoch [390/6000] | Loss D: -6.1013 | Loss G: -4.3073 | Score: 0.5583 | Critic Loss: -6.7104\n",
      "Epoch [400/6000] | Loss D: -10.2171 | Loss G: -3.5171 | Score: 0.5594 | Critic Loss: -10.8848\n",
      "Epoch [410/6000] | Loss D: -12.7048 | Loss G: -8.0219 | Score: 0.5667 | Critic Loss: -13.4150\n",
      "Epoch [420/6000] | Loss D: -6.6169 | Loss G: -11.3454 | Score: 0.5381 | Critic Loss: -7.2521\n",
      "Epoch [430/6000] | Loss D: -5.9798 | Loss G: -25.3456 | Score: 0.5652 | Critic Loss: -6.4396\n",
      "Epoch [440/6000] | Loss D: -1.1324 | Loss G: -13.4032 | Score: 0.5603 | Critic Loss: -1.7425\n",
      "Epoch [450/6000] | Loss D: -9.8792 | Loss G: -17.7070 | Score: 0.5746 | Critic Loss: -10.3775\n",
      "Epoch [460/6000] | Loss D: -8.7830 | Loss G: 3.6898 | Score: 0.5469 | Critic Loss: -9.2324\n",
      "Epoch [470/6000] | Loss D: -5.0107 | Loss G: -11.8188 | Score: 0.5462 | Critic Loss: -5.5051\n",
      "Epoch [480/6000] | Loss D: -4.8295 | Loss G: -14.1850 | Score: 0.5547 | Critic Loss: -5.1792\n",
      "Epoch [490/6000] | Loss D: -3.2637 | Loss G: -14.7286 | Score: 0.5284 | Critic Loss: -3.6934\n",
      "Epoch [500/6000] | Loss D: -4.4516 | Loss G: -23.1277 | Score: 0.5529 | Critic Loss: -4.8035\n",
      "Epoch [510/6000] | Loss D: -4.8155 | Loss G: -7.8036 | Score: 0.5379 | Critic Loss: -5.2289\n",
      "Epoch [520/6000] | Loss D: -6.0385 | Loss G: -6.8538 | Score: 0.5506 | Critic Loss: -6.5573\n",
      "Epoch [530/6000] | Loss D: -6.2206 | Loss G: -15.8643 | Score: 0.5669 | Critic Loss: -6.5913\n",
      "Epoch [540/6000] | Loss D: -6.0191 | Loss G: -11.2200 | Score: 0.5307 | Critic Loss: -6.3682\n",
      "Epoch [550/6000] | Loss D: -8.0190 | Loss G: -7.6347 | Score: 0.5552 | Critic Loss: -8.7012\n",
      "Epoch [560/6000] | Loss D: -7.0221 | Loss G: -10.2775 | Score: 0.5347 | Critic Loss: -7.5878\n",
      "Epoch [570/6000] | Loss D: -3.1805 | Loss G: -21.9404 | Score: 0.5517 | Critic Loss: -3.6098\n",
      "Epoch [580/6000] | Loss D: -5.8426 | Loss G: -14.3209 | Score: 0.5426 | Critic Loss: -6.3148\n",
      "Epoch [590/6000] | Loss D: -4.8941 | Loss G: -27.5006 | Score: 0.5299 | Critic Loss: -5.1607\n",
      "Epoch [600/6000] | Loss D: -4.1152 | Loss G: -14.2125 | Score: 0.5470 | Critic Loss: -4.5634\n",
      "Epoch [610/6000] | Loss D: -4.1915 | Loss G: -10.9079 | Score: 0.5379 | Critic Loss: -4.7561\n",
      "Epoch [620/6000] | Loss D: -6.8179 | Loss G: -28.5760 | Score: 0.5513 | Critic Loss: -7.1029\n",
      "Epoch [630/6000] | Loss D: -3.4316 | Loss G: -25.5262 | Score: 0.5441 | Critic Loss: -3.8019\n",
      "Epoch [640/6000] | Loss D: -6.9100 | Loss G: -17.7835 | Score: 0.5409 | Critic Loss: -7.4294\n",
      "Epoch [650/6000] | Loss D: -3.7507 | Loss G: -19.6054 | Score: 0.5199 | Critic Loss: -4.1488\n",
      "Epoch [660/6000] | Loss D: -3.7032 | Loss G: -15.0878 | Score: 0.5539 | Critic Loss: -4.1447\n",
      "Epoch [670/6000] | Loss D: -3.7186 | Loss G: -21.6069 | Score: 0.5379 | Critic Loss: -4.0502\n",
      "Epoch [680/6000] | Loss D: -7.5611 | Loss G: -20.5501 | Score: 0.5358 | Critic Loss: -8.0051\n",
      "Epoch [690/6000] | Loss D: -5.1195 | Loss G: -8.7282 | Score: 0.5473 | Critic Loss: -5.6761\n",
      "Epoch [700/6000] | Loss D: -2.8749 | Loss G: -25.6958 | Score: 0.5413 | Critic Loss: -3.1359\n",
      "Epoch [710/6000] | Loss D: -4.8741 | Loss G: -4.4421 | Score: 0.5320 | Critic Loss: -5.2651\n",
      "Epoch [720/6000] | Loss D: -4.7780 | Loss G: -16.8000 | Score: 0.5291 | Critic Loss: -5.3834\n",
      "Epoch [730/6000] | Loss D: -1.8311 | Loss G: -27.8201 | Score: 0.5180 | Critic Loss: -2.1022\n",
      "Epoch [740/6000] | Loss D: -4.0213 | Loss G: -22.4885 | Score: 0.5510 | Critic Loss: -4.3962\n",
      "Epoch [750/6000] | Loss D: -3.0362 | Loss G: -15.4281 | Score: 0.5091 | Critic Loss: -3.3929\n",
      "Epoch [760/6000] | Loss D: -6.2195 | Loss G: -13.6501 | Score: 0.5279 | Critic Loss: -6.6693\n",
      "Epoch [770/6000] | Loss D: -4.3243 | Loss G: -23.1419 | Score: 0.5363 | Critic Loss: -4.6216\n",
      "Epoch [780/6000] | Loss D: -2.1495 | Loss G: -31.7893 | Score: 0.5570 | Critic Loss: -2.4708\n",
      "Epoch [790/6000] | Loss D: -2.7935 | Loss G: -26.0791 | Score: 0.5217 | Critic Loss: -3.0602\n",
      "Epoch [800/6000] | Loss D: -4.7447 | Loss G: 0.8425 | Score: 0.5253 | Critic Loss: -5.2724\n",
      "Epoch [810/6000] | Loss D: -6.8876 | Loss G: -22.8927 | Score: 0.5381 | Critic Loss: -7.3761\n",
      "Epoch [820/6000] | Loss D: 1.9563 | Loss G: -31.1984 | Score: 0.5204 | Critic Loss: 1.6422\n",
      "Epoch [830/6000] | Loss D: -7.8982 | Loss G: -20.2761 | Score: 0.5214 | Critic Loss: -8.4248\n",
      "Epoch [840/6000] | Loss D: -2.7335 | Loss G: -5.6537 | Score: 0.5282 | Critic Loss: -3.4410\n",
      "Epoch [850/6000] | Loss D: -8.5807 | Loss G: -32.9123 | Score: 0.5142 | Critic Loss: -8.8945\n",
      "Epoch [860/6000] | Loss D: -9.3971 | Loss G: -31.2189 | Score: 0.5350 | Critic Loss: -9.8651\n",
      "Epoch [870/6000] | Loss D: -8.0700 | Loss G: -8.1687 | Score: 0.5297 | Critic Loss: -8.8074\n",
      "Epoch [880/6000] | Loss D: -7.7635 | Loss G: -21.4968 | Score: 0.5334 | Critic Loss: -8.7621\n",
      "Epoch [890/6000] | Loss D: -5.9092 | Loss G: -26.6651 | Score: 0.5176 | Critic Loss: -6.3808\n",
      "Epoch [900/6000] | Loss D: -7.2600 | Loss G: -28.8821 | Score: 0.5251 | Critic Loss: -7.6481\n",
      "Epoch [910/6000] | Loss D: -7.3717 | Loss G: -2.8341 | Score: 0.5145 | Critic Loss: -7.9736\n",
      "Epoch [920/6000] | Loss D: -6.7472 | Loss G: -37.0409 | Score: 0.5191 | Critic Loss: -7.1251\n",
      "Epoch [930/6000] | Loss D: -2.9017 | Loss G: -21.7773 | Score: 0.5158 | Critic Loss: -3.3785\n",
      "Epoch [940/6000] | Loss D: -6.8326 | Loss G: -23.2975 | Score: 0.5197 | Critic Loss: -7.3890\n",
      "Epoch [950/6000] | Loss D: -2.0888 | Loss G: -25.6217 | Score: 0.5160 | Critic Loss: -2.6683\n",
      "Epoch [960/6000] | Loss D: -6.2460 | Loss G: -25.3935 | Score: 0.5302 | Critic Loss: -6.6053\n",
      "Epoch [970/6000] | Loss D: -7.4777 | Loss G: -24.1265 | Score: 0.5281 | Critic Loss: -7.8587\n",
      "Epoch [980/6000] | Loss D: 0.6204 | Loss G: -32.9964 | Score: 0.5109 | Critic Loss: 0.2273\n",
      "Epoch [990/6000] | Loss D: -7.4766 | Loss G: -30.3144 | Score: 0.5261 | Critic Loss: -7.9577\n",
      "Epoch [1000/6000] | Loss D: 0.3316 | Loss G: -25.8099 | Score: 0.5272 | Critic Loss: -0.1176\n",
      "Epoch [1010/6000] | Loss D: -7.2799 | Loss G: -18.2912 | Score: 0.5307 | Critic Loss: -7.6450\n",
      "Epoch [1020/6000] | Loss D: -7.6736 | Loss G: -44.3940 | Score: 0.5225 | Critic Loss: -8.0300\n",
      "Epoch [1030/6000] | Loss D: -7.5358 | Loss G: -33.5251 | Score: 0.5233 | Critic Loss: -7.9740\n",
      "Epoch [1040/6000] | Loss D: -5.1831 | Loss G: -10.0243 | Score: 0.5197 | Critic Loss: -5.7067\n",
      "Epoch [1050/6000] | Loss D: -8.3582 | Loss G: -32.4538 | Score: 0.5215 | Critic Loss: -8.8270\n",
      "Epoch [1060/6000] | Loss D: -2.8695 | Loss G: -37.2829 | Score: 0.5064 | Critic Loss: -3.2246\n",
      "Epoch [1070/6000] | Loss D: -7.1522 | Loss G: -24.4243 | Score: 0.5134 | Critic Loss: -7.5611\n",
      "Epoch [1080/6000] | Loss D: -4.9965 | Loss G: -20.4587 | Score: 0.5102 | Critic Loss: -5.5344\n",
      "Epoch [1090/6000] | Loss D: -10.3763 | Loss G: -25.0799 | Score: 0.5225 | Critic Loss: -10.7579\n",
      "Epoch [1100/6000] | Loss D: -3.1328 | Loss G: -36.0408 | Score: 0.5132 | Critic Loss: -3.6002\n",
      "Epoch [1110/6000] | Loss D: -7.7653 | Loss G: -23.2498 | Score: 0.5237 | Critic Loss: -8.2764\n",
      "Epoch [1120/6000] | Loss D: -10.9467 | Loss G: -16.1310 | Score: 0.5097 | Critic Loss: -11.6242\n",
      "Epoch [1130/6000] | Loss D: -2.0532 | Loss G: -35.0694 | Score: 0.5285 | Critic Loss: -2.3768\n",
      "Epoch [1140/6000] | Loss D: -8.8224 | Loss G: -18.7471 | Score: 0.5208 | Critic Loss: -9.3887\n",
      "Epoch [1150/6000] | Loss D: -2.0839 | Loss G: -19.5672 | Score: 0.5086 | Critic Loss: -2.4757\n",
      "Epoch [1160/6000] | Loss D: -8.2826 | Loss G: -37.2679 | Score: 0.5119 | Critic Loss: -8.9085\n",
      "Epoch [1170/6000] | Loss D: -1.9765 | Loss G: -30.0611 | Score: 0.5220 | Critic Loss: -2.4789\n",
      "Epoch [1180/6000] | Loss D: -9.8283 | Loss G: -27.8505 | Score: 0.5159 | Critic Loss: -10.1944\n",
      "Epoch [1190/6000] | Loss D: -10.6339 | Loss G: -28.0194 | Score: 0.5160 | Critic Loss: -11.2824\n",
      "Epoch [1200/6000] | Loss D: -1.6689 | Loss G: -27.4790 | Score: 0.5045 | Critic Loss: -2.2843\n",
      "Epoch [1210/6000] | Loss D: -14.3152 | Loss G: -38.7104 | Score: 0.5299 | Critic Loss: -14.7894\n",
      "Epoch [1220/6000] | Loss D: -2.7832 | Loss G: -14.9673 | Score: 0.5307 | Critic Loss: -3.3772\n",
      "Epoch [1230/6000] | Loss D: -9.8720 | Loss G: -35.0157 | Score: 0.5139 | Critic Loss: -10.3721\n",
      "Epoch [1240/6000] | Loss D: -4.6263 | Loss G: -36.5190 | Score: 0.5013 | Critic Loss: -4.9697\n",
      "Epoch [1250/6000] | Loss D: -13.0752 | Loss G: -24.2375 | Score: 0.5070 | Critic Loss: -13.7744\n",
      "Epoch [1260/6000] | Loss D: -0.4490 | Loss G: -27.5619 | Score: 0.5102 | Critic Loss: -0.8929\n",
      "Epoch [1270/6000] | Loss D: -12.4571 | Loss G: -44.8964 | Score: 0.5221 | Critic Loss: -12.8643\n",
      "Epoch [1280/6000] | Loss D: -4.5265 | Loss G: -24.0829 | Score: 0.5281 | Critic Loss: -5.0052\n",
      "Epoch [1290/6000] | Loss D: -5.5177 | Loss G: -23.9548 | Score: 0.4964 | Critic Loss: -6.0638\n",
      "Epoch [1300/6000] | Loss D: -4.2295 | Loss G: -35.5866 | Score: 0.5271 | Critic Loss: -4.6089\n",
      "Epoch [1310/6000] | Loss D: -6.9377 | Loss G: -27.5403 | Score: 0.5028 | Critic Loss: -7.3739\n",
      "Epoch [1320/6000] | Loss D: -11.0567 | Loss G: -29.4529 | Score: 0.5215 | Critic Loss: -11.7898\n",
      "Epoch [1330/6000] | Loss D: -3.1950 | Loss G: -28.0651 | Score: 0.5094 | Critic Loss: -3.5222\n",
      "Epoch [1340/6000] | Loss D: -9.6251 | Loss G: -47.8613 | Score: 0.5004 | Critic Loss: -10.0985\n",
      "Epoch [1350/6000] | Loss D: -9.9504 | Loss G: -23.4245 | Score: 0.5148 | Critic Loss: -10.8246\n",
      "Epoch [1360/6000] | Loss D: -5.0494 | Loss G: -38.7213 | Score: 0.5131 | Critic Loss: -5.5034\n",
      "Epoch [1370/6000] | Loss D: -11.7069 | Loss G: -51.3675 | Score: 0.5258 | Critic Loss: -11.9608\n",
      "Epoch [1380/6000] | Loss D: -7.3026 | Loss G: -24.9719 | Score: 0.5188 | Critic Loss: -7.8761\n",
      "Epoch [1390/6000] | Loss D: -4.1733 | Loss G: -40.5198 | Score: 0.5036 | Critic Loss: -4.5997\n",
      "Epoch [1400/6000] | Loss D: -8.5355 | Loss G: -23.5863 | Score: 0.5104 | Critic Loss: -9.7265\n",
      "Epoch [1410/6000] | Loss D: -3.8096 | Loss G: -52.3696 | Score: 0.5302 | Critic Loss: -4.2061\n",
      "Epoch [1420/6000] | Loss D: -3.5477 | Loss G: -41.3265 | Score: 0.5026 | Critic Loss: -3.9483\n",
      "Epoch [1430/6000] | Loss D: -2.6126 | Loss G: -37.7555 | Score: 0.5345 | Critic Loss: -3.3058\n",
      "Epoch [1440/6000] | Loss D: -5.7631 | Loss G: -46.2393 | Score: 0.5062 | Critic Loss: -6.4231\n",
      "Epoch [1450/6000] | Loss D: -7.4530 | Loss G: -38.8860 | Score: 0.5190 | Critic Loss: -7.9752\n",
      "Epoch [1460/6000] | Loss D: -6.4719 | Loss G: -31.5078 | Score: 0.5221 | Critic Loss: -7.0588\n",
      "Epoch [1470/6000] | Loss D: -5.9774 | Loss G: -36.0502 | Score: 0.5036 | Critic Loss: -6.3828\n",
      "Epoch [1480/6000] | Loss D: -7.2751 | Loss G: -45.5929 | Score: 0.5159 | Critic Loss: -7.8511\n",
      "Epoch [1490/6000] | Loss D: -1.5026 | Loss G: -33.1689 | Score: 0.5083 | Critic Loss: -1.8898\n",
      "Epoch [1500/6000] | Loss D: -8.5573 | Loss G: -36.8572 | Score: 0.5160 | Critic Loss: -9.0122\n",
      "Epoch [1510/6000] | Loss D: -8.0486 | Loss G: -35.4935 | Score: 0.5148 | Critic Loss: -8.6402\n",
      "Epoch [1520/6000] | Loss D: -3.1896 | Loss G: -34.7839 | Score: 0.5140 | Critic Loss: -3.6928\n",
      "Epoch [1530/6000] | Loss D: -13.3351 | Loss G: -50.0487 | Score: 0.5185 | Critic Loss: -13.6272\n",
      "Epoch [1540/6000] | Loss D: -16.1995 | Loss G: -21.6855 | Score: 0.5220 | Critic Loss: -17.0747\n",
      "Epoch [1550/6000] | Loss D: -4.2677 | Loss G: -45.2000 | Score: 0.5139 | Critic Loss: -5.1895\n",
      "Epoch [1560/6000] | Loss D: -3.9505 | Loss G: -23.8528 | Score: 0.5279 | Critic Loss: -4.7160\n",
      "Epoch [1570/6000] | Loss D: -8.1522 | Loss G: -45.7429 | Score: 0.5094 | Critic Loss: -8.9147\n",
      "Epoch [1580/6000] | Loss D: -4.5249 | Loss G: -32.3526 | Score: 0.5191 | Critic Loss: -4.9753\n",
      "Epoch [1590/6000] | Loss D: -6.7252 | Loss G: -41.7967 | Score: 0.5039 | Critic Loss: -7.2965\n",
      "Epoch [1600/6000] | Loss D: -9.7278 | Loss G: -39.0074 | Score: 0.4955 | Critic Loss: -10.2833\n",
      "Epoch [1610/6000] | Loss D: -2.9019 | Loss G: -35.8625 | Score: 0.5165 | Critic Loss: -3.4497\n",
      "Epoch [1620/6000] | Loss D: -8.8447 | Loss G: -37.2690 | Score: 0.5028 | Critic Loss: -9.5748\n",
      "Epoch [1630/6000] | Loss D: -10.2695 | Loss G: -56.6449 | Score: 0.5099 | Critic Loss: -10.9393\n",
      "Epoch [1640/6000] | Loss D: -11.7370 | Loss G: -27.2537 | Score: 0.4975 | Critic Loss: -12.1579\n",
      "Epoch [1650/6000] | Loss D: -4.7394 | Loss G: -47.4211 | Score: 0.5121 | Critic Loss: -5.3372\n",
      "Epoch [1660/6000] | Loss D: -3.4594 | Loss G: -32.9041 | Score: 0.5171 | Critic Loss: -3.9439\n",
      "Epoch [1670/6000] | Loss D: -5.5588 | Loss G: -36.1122 | Score: 0.5202 | Critic Loss: -5.9104\n",
      "Epoch [1680/6000] | Loss D: -8.5376 | Loss G: -54.5248 | Score: 0.5113 | Critic Loss: -9.0283\n",
      "Epoch [1690/6000] | Loss D: -7.0783 | Loss G: -12.7696 | Score: 0.5095 | Critic Loss: -7.8495\n",
      "Epoch [1700/6000] | Loss D: -2.7309 | Loss G: -38.3000 | Score: 0.5321 | Critic Loss: -3.5583\n",
      "Epoch [1710/6000] | Loss D: -10.8068 | Loss G: -49.3535 | Score: 0.5022 | Critic Loss: -11.6077\n",
      "Epoch [1720/6000] | Loss D: -5.6474 | Loss G: -50.6077 | Score: 0.5103 | Critic Loss: -6.4475\n",
      "Epoch [1730/6000] | Loss D: -7.4393 | Loss G: -49.9499 | Score: 0.5270 | Critic Loss: -7.9130\n",
      "Epoch [1740/6000] | Loss D: -6.5239 | Loss G: -36.8564 | Score: 0.5037 | Critic Loss: -7.2781\n",
      "Epoch [1750/6000] | Loss D: -4.9273 | Loss G: -65.5920 | Score: 0.5048 | Critic Loss: -5.4479\n",
      "Epoch [1760/6000] | Loss D: -10.1291 | Loss G: -36.4731 | Score: 0.5253 | Critic Loss: -10.9296\n",
      "Epoch [1770/6000] | Loss D: -6.8783 | Loss G: -37.9674 | Score: 0.5096 | Critic Loss: -7.6454\n",
      "Epoch [1780/6000] | Loss D: -9.3914 | Loss G: -61.8290 | Score: 0.5269 | Critic Loss: -10.0300\n",
      "Epoch [1790/6000] | Loss D: -9.9420 | Loss G: -43.7043 | Score: 0.5085 | Critic Loss: -10.4674\n",
      "Epoch [1800/6000] | Loss D: -15.3525 | Loss G: -50.0950 | Score: 0.5150 | Critic Loss: -16.5710\n",
      "Epoch [1810/6000] | Loss D: -2.4879 | Loss G: -32.6511 | Score: 0.4920 | Critic Loss: -2.9725\n",
      "Epoch [1820/6000] | Loss D: -10.9970 | Loss G: -48.5986 | Score: 0.5068 | Critic Loss: -11.5902\n",
      "Epoch [1830/6000] | Loss D: -9.9937 | Loss G: -20.9794 | Score: 0.5220 | Critic Loss: -10.7287\n",
      "Epoch [1840/6000] | Loss D: -2.7667 | Loss G: -58.4111 | Score: 0.4952 | Critic Loss: -3.4058\n",
      "Epoch [1850/6000] | Loss D: -6.8509 | Loss G: -49.7669 | Score: 0.5244 | Critic Loss: -7.5033\n",
      "Epoch [1860/6000] | Loss D: -14.1475 | Loss G: -44.7040 | Score: 0.4981 | Critic Loss: -15.0096\n",
      "Epoch [1870/6000] | Loss D: -5.1881 | Loss G: -34.2017 | Score: 0.5184 | Critic Loss: -6.2148\n",
      "Epoch [1880/6000] | Loss D: -6.6296 | Loss G: -56.2550 | Score: 0.4993 | Critic Loss: -7.0549\n",
      "Epoch [1890/6000] | Loss D: -13.8343 | Loss G: -38.8724 | Score: 0.5051 | Critic Loss: -14.6244\n",
      "Epoch [1900/6000] | Loss D: -5.3003 | Loss G: -48.8719 | Score: 0.5148 | Critic Loss: -5.9084\n",
      "Epoch [1910/6000] | Loss D: -3.4432 | Loss G: -53.1309 | Score: 0.5181 | Critic Loss: -3.9710\n",
      "Epoch [1920/6000] | Loss D: -17.3643 | Loss G: -35.7414 | Score: 0.5059 | Critic Loss: -18.1939\n",
      "Epoch [1930/6000] | Loss D: -2.4568 | Loss G: -53.2498 | Score: 0.4923 | Critic Loss: -3.2078\n",
      "Epoch [1940/6000] | Loss D: -11.3252 | Loss G: -37.6564 | Score: 0.5237 | Critic Loss: -12.1517\n",
      "Epoch [1950/6000] | Loss D: -4.1925 | Loss G: -60.3655 | Score: 0.4918 | Critic Loss: -4.9528\n",
      "Epoch [1960/6000] | Loss D: -12.7221 | Loss G: -52.1363 | Score: 0.5041 | Critic Loss: -13.3584\n",
      "Epoch [1970/6000] | Loss D: -7.0730 | Loss G: -43.6620 | Score: 0.5045 | Critic Loss: -7.7963\n",
      "Epoch [1980/6000] | Loss D: -8.9948 | Loss G: -52.5163 | Score: 0.5141 | Critic Loss: -9.6147\n",
      "Epoch [1990/6000] | Loss D: -2.2127 | Loss G: -50.2748 | Score: 0.5068 | Critic Loss: -2.7300\n",
      "Epoch [2000/6000] | Loss D: -9.0086 | Loss G: -28.9367 | Score: 0.4913 | Critic Loss: -9.7372\n",
      "Epoch [2010/6000] | Loss D: -11.6365 | Loss G: -64.0390 | Score: 0.5203 | Critic Loss: -12.0623\n",
      "Epoch [2020/6000] | Loss D: -7.9968 | Loss G: -42.6609 | Score: 0.4995 | Critic Loss: -8.3890\n",
      "Epoch [2030/6000] | Loss D: -2.7596 | Loss G: -46.3135 | Score: 0.4924 | Critic Loss: -3.3883\n",
      "Epoch [2040/6000] | Loss D: -9.8466 | Loss G: -58.4478 | Score: 0.4953 | Critic Loss: -10.4822\n",
      "Epoch [2050/6000] | Loss D: -6.9114 | Loss G: -47.9350 | Score: 0.4944 | Critic Loss: -7.7025\n",
      "Epoch [2060/6000] | Loss D: -4.2100 | Loss G: -59.7110 | Score: 0.5056 | Critic Loss: -4.5894\n",
      "Epoch [2070/6000] | Loss D: -12.9629 | Loss G: -47.5466 | Score: 0.4988 | Critic Loss: -13.8226\n",
      "Epoch [2080/6000] | Loss D: -10.6293 | Loss G: -54.1737 | Score: 0.4994 | Critic Loss: -11.3186\n",
      "Epoch [2090/6000] | Loss D: -11.5911 | Loss G: -43.9154 | Score: 0.5241 | Critic Loss: -12.5938\n",
      "Epoch [2100/6000] | Loss D: -6.7920 | Loss G: -67.8645 | Score: 0.4972 | Critic Loss: -7.2631\n",
      "Epoch [2110/6000] | Loss D: -10.3688 | Loss G: -55.8364 | Score: 0.5150 | Critic Loss: -11.4829\n",
      "Epoch [2120/6000] | Loss D: -7.5026 | Loss G: -55.3344 | Score: 0.5143 | Critic Loss: -8.4234\n",
      "Epoch [2130/6000] | Loss D: -12.3062 | Loss G: -57.1031 | Score: 0.5143 | Critic Loss: -13.2900\n",
      "Epoch [2140/6000] | Loss D: -6.6212 | Loss G: -53.3470 | Score: 0.5011 | Critic Loss: -7.6017\n",
      "Epoch [2150/6000] | Loss D: -11.5285 | Loss G: -59.7086 | Score: 0.5116 | Critic Loss: -12.2575\n",
      "Epoch [2160/6000] | Loss D: -11.0033 | Loss G: -63.1707 | Score: 0.5001 | Critic Loss: -11.6092\n",
      "Epoch [2170/6000] | Loss D: -10.2734 | Loss G: -49.7941 | Score: 0.5063 | Critic Loss: -11.1654\n",
      "Epoch [2180/6000] | Loss D: -4.3875 | Loss G: -62.1666 | Score: 0.4961 | Critic Loss: -4.8840\n",
      "Epoch [2190/6000] | Loss D: -12.4967 | Loss G: -64.5573 | Score: 0.5062 | Critic Loss: -12.9871\n",
      "Epoch [2200/6000] | Loss D: -16.1793 | Loss G: -55.1135 | Score: 0.5092 | Critic Loss: -16.8613\n",
      "Epoch [2210/6000] | Loss D: -5.3772 | Loss G: -42.9042 | Score: 0.5111 | Critic Loss: -6.2809\n",
      "Epoch [2220/6000] | Loss D: -6.2454 | Loss G: -61.7441 | Score: 0.5015 | Critic Loss: -6.8439\n",
      "Epoch [2230/6000] | Loss D: -6.5997 | Loss G: -55.8236 | Score: 0.5061 | Critic Loss: -7.1637\n",
      "Epoch [2240/6000] | Loss D: -11.5976 | Loss G: -59.4428 | Score: 0.5153 | Critic Loss: -12.2627\n",
      "Epoch [2250/6000] | Loss D: -4.8237 | Loss G: -52.2271 | Score: 0.4987 | Critic Loss: -5.7252\n",
      "Epoch [2260/6000] | Loss D: -9.6084 | Loss G: -58.8795 | Score: 0.5222 | Critic Loss: -10.3679\n",
      "Epoch [2270/6000] | Loss D: -5.9260 | Loss G: -74.9767 | Score: 0.5115 | Critic Loss: -6.5317\n",
      "Epoch [2280/6000] | Loss D: -12.2878 | Loss G: -40.8300 | Score: 0.4917 | Critic Loss: -13.3080\n",
      "Epoch [2290/6000] | Loss D: -5.5012 | Loss G: -68.2096 | Score: 0.4995 | Critic Loss: -6.0879\n",
      "Epoch [2300/6000] | Loss D: -7.6278 | Loss G: -53.2324 | Score: 0.5174 | Critic Loss: -8.7281\n",
      "Epoch [2310/6000] | Loss D: -13.0816 | Loss G: -69.9186 | Score: 0.5023 | Critic Loss: -13.5277\n",
      "Epoch [2320/6000] | Loss D: -8.7588 | Loss G: -50.4809 | Score: 0.5002 | Critic Loss: -9.5632\n",
      "Epoch [2330/6000] | Loss D: -9.6488 | Loss G: -75.2882 | Score: 0.4994 | Critic Loss: -10.2778\n",
      "Epoch [2340/6000] | Loss D: -7.4368 | Loss G: -43.5217 | Score: 0.5029 | Critic Loss: -8.4425\n",
      "Epoch [2350/6000] | Loss D: -13.2154 | Loss G: -54.7246 | Score: 0.4996 | Critic Loss: -13.9281\n",
      "Epoch [2360/6000] | Loss D: -9.6727 | Loss G: -67.8227 | Score: 0.5066 | Critic Loss: -10.5950\n",
      "Epoch [2370/6000] | Loss D: -9.0837 | Loss G: -58.1642 | Score: 0.4950 | Critic Loss: -9.7262\n",
      "Epoch [2380/6000] | Loss D: -15.0207 | Loss G: -49.2149 | Score: 0.5080 | Critic Loss: -16.0372\n",
      "Epoch [2390/6000] | Loss D: -10.1325 | Loss G: -67.9159 | Score: 0.5059 | Critic Loss: -10.7831\n",
      "Epoch [2400/6000] | Loss D: -13.8409 | Loss G: -64.9312 | Score: 0.4995 | Critic Loss: -14.5978\n",
      "Epoch [2410/6000] | Loss D: -0.9746 | Loss G: -72.2266 | Score: 0.5055 | Critic Loss: -1.7321\n",
      "Epoch [2420/6000] | Loss D: -10.0238 | Loss G: -60.9212 | Score: 0.5081 | Critic Loss: -11.1811\n",
      "Epoch [2430/6000] | Loss D: -12.2820 | Loss G: -72.8233 | Score: 0.5094 | Critic Loss: -13.2070\n",
      "Epoch [2440/6000] | Loss D: -5.5613 | Loss G: -51.9169 | Score: 0.5080 | Critic Loss: -6.6412\n",
      "Epoch [2450/6000] | Loss D: -6.6463 | Loss G: -68.1390 | Score: 0.4968 | Critic Loss: -7.4765\n",
      "Epoch [2460/6000] | Loss D: -10.1670 | Loss G: -57.2258 | Score: 0.5031 | Critic Loss: -11.2463\n",
      "Epoch [2470/6000] | Loss D: -12.7376 | Loss G: -75.0515 | Score: 0.5008 | Critic Loss: -13.3160\n",
      "Epoch [2480/6000] | Loss D: -9.6984 | Loss G: -61.1248 | Score: 0.4980 | Critic Loss: -10.7957\n",
      "Epoch [2490/6000] | Loss D: -7.1958 | Loss G: -55.3571 | Score: 0.5129 | Critic Loss: -8.2069\n",
      "Epoch [2500/6000] | Loss D: -8.7020 | Loss G: -55.7782 | Score: 0.4982 | Critic Loss: -9.9929\n",
      "Epoch [2510/6000] | Loss D: -11.4706 | Loss G: -58.7047 | Score: 0.5068 | Critic Loss: -12.2296\n",
      "Epoch [2520/6000] | Loss D: -14.2630 | Loss G: -70.8806 | Score: 0.4969 | Critic Loss: -15.0206\n",
      "Epoch [2530/6000] | Loss D: -9.0278 | Loss G: -69.2447 | Score: 0.4990 | Critic Loss: -9.7381\n",
      "Epoch [2540/6000] | Loss D: -9.7916 | Loss G: -76.1280 | Score: 0.5095 | Critic Loss: -10.4694\n",
      "Epoch [2550/6000] | Loss D: -13.0723 | Loss G: -61.0820 | Score: 0.5011 | Critic Loss: -13.8924\n",
      "Epoch [2560/6000] | Loss D: -11.2349 | Loss G: -76.9617 | Score: 0.5086 | Critic Loss: -12.6470\n",
      "Epoch [2570/6000] | Loss D: -9.1673 | Loss G: -61.2344 | Score: 0.4985 | Critic Loss: -10.0413\n",
      "Epoch [2580/6000] | Loss D: -10.5637 | Loss G: -59.4759 | Score: 0.5151 | Critic Loss: -11.3480\n",
      "Epoch [2590/6000] | Loss D: -11.8900 | Loss G: -90.0066 | Score: 0.5060 | Critic Loss: -12.7966\n",
      "Epoch [2600/6000] | Loss D: -9.5206 | Loss G: -61.3292 | Score: 0.5076 | Critic Loss: -10.3047\n",
      "Epoch [2610/6000] | Loss D: -10.0570 | Loss G: -72.0575 | Score: 0.4989 | Critic Loss: -10.6365\n",
      "Epoch [2620/6000] | Loss D: -11.7608 | Loss G: -69.5702 | Score: 0.5021 | Critic Loss: -12.9591\n",
      "Epoch [2630/6000] | Loss D: -7.0534 | Loss G: -77.4284 | Score: 0.5121 | Critic Loss: -8.1170\n",
      "Epoch [2640/6000] | Loss D: -4.2577 | Loss G: -43.3857 | Score: 0.5036 | Critic Loss: -5.1009\n",
      "Epoch [2650/6000] | Loss D: -13.1601 | Loss G: -95.3533 | Score: 0.5109 | Critic Loss: -13.7544\n",
      "Epoch [2660/6000] | Loss D: -11.1045 | Loss G: -53.7982 | Score: 0.4999 | Critic Loss: -12.3454\n",
      "Epoch [2670/6000] | Loss D: -10.4221 | Loss G: -83.0274 | Score: 0.5083 | Critic Loss: -11.0483\n",
      "Epoch [2680/6000] | Loss D: -13.6172 | Loss G: -68.0642 | Score: 0.5105 | Critic Loss: -14.6222\n",
      "Epoch [2690/6000] | Loss D: -12.6744 | Loss G: -76.1994 | Score: 0.5116 | Critic Loss: -13.7169\n",
      "Epoch [2700/6000] | Loss D: -5.5318 | Loss G: -63.9553 | Score: 0.5092 | Critic Loss: -6.2800\n",
      "Epoch [2710/6000] | Loss D: -12.5205 | Loss G: -81.7615 | Score: 0.5035 | Critic Loss: -13.1818\n",
      "Epoch [2720/6000] | Loss D: -5.3446 | Loss G: -74.7824 | Score: 0.5033 | Critic Loss: -6.1582\n",
      "Epoch [2730/6000] | Loss D: -9.2134 | Loss G: -67.6242 | Score: 0.5144 | Critic Loss: -9.9227\n",
      "Epoch [2740/6000] | Loss D: -13.0374 | Loss G: -73.5879 | Score: 0.5083 | Critic Loss: -13.9877\n",
      "Epoch [2750/6000] | Loss D: -10.3819 | Loss G: -79.0941 | Score: 0.4993 | Critic Loss: -11.0199\n",
      "Epoch [2760/6000] | Loss D: -14.4726 | Loss G: -73.6054 | Score: 0.4969 | Critic Loss: -15.3548\n",
      "Epoch [2770/6000] | Loss D: -3.3868 | Loss G: -69.0738 | Score: 0.5000 | Critic Loss: -4.1524\n",
      "Epoch [2780/6000] | Loss D: -7.6184 | Loss G: -65.0755 | Score: 0.5014 | Critic Loss: -8.4869\n",
      "Epoch [2790/6000] | Loss D: -7.5647 | Loss G: -78.2123 | Score: 0.4971 | Critic Loss: -8.3254\n",
      "Epoch [2800/6000] | Loss D: -10.3362 | Loss G: -69.3832 | Score: 0.4933 | Critic Loss: -11.0755\n",
      "Epoch [2810/6000] | Loss D: -9.3860 | Loss G: -83.9001 | Score: 0.5123 | Critic Loss: -10.0879\n",
      "Epoch [2820/6000] | Loss D: -9.7128 | Loss G: -63.4809 | Score: 0.5133 | Critic Loss: -10.5850\n",
      "Epoch [2830/6000] | Loss D: -6.1639 | Loss G: -81.3329 | Score: 0.5059 | Critic Loss: -7.4696\n",
      "Epoch [2840/6000] | Loss D: -11.7286 | Loss G: -91.4919 | Score: 0.5048 | Critic Loss: -12.5731\n",
      "Epoch [2850/6000] | Loss D: -9.6579 | Loss G: -88.8022 | Score: 0.5050 | Critic Loss: -10.3818\n",
      "Epoch [2860/6000] | Loss D: -6.1239 | Loss G: -74.9897 | Score: 0.5060 | Critic Loss: -6.9618\n",
      "Epoch [2870/6000] | Loss D: -9.6180 | Loss G: -93.2329 | Score: 0.5103 | Critic Loss: -10.2870\n",
      "Epoch [2880/6000] | Loss D: -9.1872 | Loss G: -88.8397 | Score: 0.5125 | Critic Loss: -10.2477\n",
      "Epoch [2890/6000] | Loss D: -8.5132 | Loss G: -72.6921 | Score: 0.5055 | Critic Loss: -9.5470\n",
      "Epoch [2900/6000] | Loss D: -6.6463 | Loss G: -60.2840 | Score: 0.5102 | Critic Loss: -8.0099\n",
      "Epoch [2910/6000] | Loss D: -8.6452 | Loss G: -76.3932 | Score: 0.5119 | Critic Loss: -9.6475\n",
      "Epoch [2920/6000] | Loss D: -14.2045 | Loss G: -60.0891 | Score: 0.4989 | Critic Loss: -15.3569\n",
      "Epoch [2930/6000] | Loss D: -7.9983 | Loss G: -70.4916 | Score: 0.5098 | Critic Loss: -9.0047\n",
      "Epoch [2940/6000] | Loss D: -10.6129 | Loss G: -53.2650 | Score: 0.4997 | Critic Loss: -11.6313\n",
      "Epoch [2950/6000] | Loss D: -18.2655 | Loss G: -75.8159 | Score: 0.5034 | Critic Loss: -19.1991\n",
      "Epoch [2960/6000] | Loss D: -2.2906 | Loss G: -96.0456 | Score: 0.4939 | Critic Loss: -3.1839\n",
      "Epoch [2970/6000] | Loss D: -11.1861 | Loss G: -78.0044 | Score: 0.5024 | Critic Loss: -12.2185\n",
      "Epoch [2980/6000] | Loss D: -11.5279 | Loss G: -69.8568 | Score: 0.5060 | Critic Loss: -13.0918\n",
      "Epoch [2990/6000] | Loss D: -12.1916 | Loss G: -83.4753 | Score: 0.5042 | Critic Loss: -13.2780\n",
      "Epoch [3000/6000] | Loss D: -16.9194 | Loss G: -77.5285 | Score: 0.5032 | Critic Loss: -17.9023\n",
      "Epoch [3010/6000] | Loss D: -14.4489 | Loss G: -57.2093 | Score: 0.5026 | Critic Loss: -15.6893\n",
      "Epoch [3020/6000] | Loss D: -14.1230 | Loss G: -79.4676 | Score: 0.5104 | Critic Loss: -15.3060\n",
      "Epoch [3030/6000] | Loss D: -18.1631 | Loss G: -96.1484 | Score: 0.5085 | Critic Loss: -19.2778\n",
      "Epoch [3040/6000] | Loss D: -6.1597 | Loss G: -55.5479 | Score: 0.4952 | Critic Loss: -7.0741\n",
      "Epoch [3050/6000] | Loss D: -14.1895 | Loss G: -80.1473 | Score: 0.4959 | Critic Loss: -15.0697\n",
      "Epoch [3060/6000] | Loss D: -18.2785 | Loss G: -73.3284 | Score: 0.5167 | Critic Loss: -19.5499\n",
      "Epoch [3070/6000] | Loss D: -5.7255 | Loss G: -59.5038 | Score: 0.4961 | Critic Loss: -6.7871\n",
      "Epoch [3080/6000] | Loss D: -15.1600 | Loss G: -89.6444 | Score: 0.4979 | Critic Loss: -15.7745\n",
      "Epoch [3090/6000] | Loss D: -10.5414 | Loss G: -71.4066 | Score: 0.4960 | Critic Loss: -11.8450\n",
      "Epoch [3100/6000] | Loss D: -7.3680 | Loss G: -86.0444 | Score: 0.4985 | Critic Loss: -8.1801\n",
      "Epoch [3110/6000] | Loss D: -18.4595 | Loss G: -74.8082 | Score: 0.4984 | Critic Loss: -19.6785\n",
      "Epoch [3120/6000] | Loss D: -8.0482 | Loss G: -79.5444 | Score: 0.5020 | Critic Loss: -9.1639\n",
      "Epoch [3130/6000] | Loss D: -18.6062 | Loss G: -79.0527 | Score: 0.4956 | Critic Loss: -19.4219\n",
      "Epoch [3140/6000] | Loss D: -5.1800 | Loss G: -82.9831 | Score: 0.5001 | Critic Loss: -6.0262\n",
      "Epoch [3150/6000] | Loss D: -10.7228 | Loss G: -89.2024 | Score: 0.5050 | Critic Loss: -11.6693\n",
      "Epoch [3160/6000] | Loss D: -11.3108 | Loss G: -75.7985 | Score: 0.5059 | Critic Loss: -12.3987\n",
      "Epoch [3170/6000] | Loss D: -9.9881 | Loss G: -78.0330 | Score: 0.5064 | Critic Loss: -10.9357\n",
      "Epoch [3180/6000] | Loss D: -6.5986 | Loss G: -79.6039 | Score: 0.4918 | Critic Loss: -7.6484\n",
      "Epoch [3190/6000] | Loss D: -6.0134 | Loss G: -87.8661 | Score: 0.5065 | Critic Loss: -7.0162\n",
      "Epoch [3200/6000] | Loss D: -7.5456 | Loss G: -86.3582 | Score: 0.4990 | Critic Loss: -8.4579\n",
      "Epoch [3210/6000] | Loss D: -4.8275 | Loss G: -68.5702 | Score: 0.5015 | Critic Loss: -5.9439\n",
      "Epoch [3220/6000] | Loss D: -13.9221 | Loss G: -82.1917 | Score: 0.5045 | Critic Loss: -14.9590\n",
      "Epoch [3230/6000] | Loss D: -10.7091 | Loss G: -77.1272 | Score: 0.5037 | Critic Loss: -11.8495\n",
      "Epoch [3240/6000] | Loss D: -13.2513 | Loss G: -92.1511 | Score: 0.5037 | Critic Loss: -14.1097\n",
      "Epoch [3250/6000] | Loss D: -9.7774 | Loss G: -49.5440 | Score: 0.5069 | Critic Loss: -11.5763\n",
      "Epoch [3260/6000] | Loss D: -9.0371 | Loss G: -87.6709 | Score: 0.4977 | Critic Loss: -9.9366\n",
      "Epoch [3270/6000] | Loss D: -13.6566 | Loss G: -84.0035 | Score: 0.4953 | Critic Loss: -15.2721\n",
      "Epoch [3280/6000] | Loss D: -8.2144 | Loss G: -69.5842 | Score: 0.4973 | Critic Loss: -9.5128\n",
      "Epoch [3290/6000] | Loss D: -16.0922 | Loss G: -73.2995 | Score: 0.5008 | Critic Loss: -17.7568\n",
      "Epoch [3300/6000] | Loss D: -12.7844 | Loss G: -71.0593 | Score: 0.4958 | Critic Loss: -14.4445\n",
      "Epoch [3310/6000] | Loss D: -12.7668 | Loss G: -87.3731 | Score: 0.5010 | Critic Loss: -13.8467\n",
      "Epoch [3320/6000] | Loss D: -5.8842 | Loss G: -66.6399 | Score: 0.5037 | Critic Loss: -7.4397\n",
      "Epoch [3330/6000] | Loss D: -13.6117 | Loss G: -74.4195 | Score: 0.5028 | Critic Loss: -14.7206\n",
      "Epoch [3340/6000] | Loss D: -13.6713 | Loss G: -82.6272 | Score: 0.4943 | Critic Loss: -14.8634\n",
      "Epoch [3350/6000] | Loss D: -7.9833 | Loss G: -91.3388 | Score: 0.5028 | Critic Loss: -8.9638\n",
      "Epoch [3360/6000] | Loss D: -12.9804 | Loss G: -83.0343 | Score: 0.4985 | Critic Loss: -13.8899\n",
      "Epoch [3370/6000] | Loss D: -16.1623 | Loss G: -95.9383 | Score: 0.4991 | Critic Loss: -17.0891\n",
      "Epoch [3380/6000] | Loss D: -4.3267 | Loss G: -77.1974 | Score: 0.4935 | Critic Loss: -5.2697\n",
      "Epoch [3390/6000] | Loss D: -10.3590 | Loss G: -88.1013 | Score: 0.5017 | Critic Loss: -11.3617\n",
      "Epoch [3400/6000] | Loss D: -13.9085 | Loss G: -83.5921 | Score: 0.5024 | Critic Loss: -14.9584\n",
      "Epoch [3410/6000] | Loss D: -7.8653 | Loss G: -104.5176 | Score: 0.4996 | Critic Loss: -8.7741\n",
      "Epoch [3420/6000] | Loss D: -12.0198 | Loss G: -88.8273 | Score: 0.4939 | Critic Loss: -12.9152\n",
      "Epoch [3430/6000] | Loss D: -9.6008 | Loss G: -83.1485 | Score: 0.4998 | Critic Loss: -11.0939\n",
      "Epoch [3440/6000] | Loss D: -16.6827 | Loss G: -84.2250 | Score: 0.5058 | Critic Loss: -17.8585\n",
      "Epoch [3450/6000] | Loss D: -10.7802 | Loss G: -85.5436 | Score: 0.4911 | Critic Loss: -11.6992\n",
      "Epoch [3460/6000] | Loss D: -1.1999 | Loss G: -65.0009 | Score: 0.5061 | Critic Loss: -2.3307\n",
      "Epoch [3470/6000] | Loss D: -12.4109 | Loss G: -87.9106 | Score: 0.4855 | Critic Loss: -13.7050\n",
      "Epoch [3480/6000] | Loss D: -14.1248 | Loss G: -92.0742 | Score: 0.4970 | Critic Loss: -15.5967\n",
      "Epoch [3490/6000] | Loss D: -9.9909 | Loss G: -78.4328 | Score: 0.4968 | Critic Loss: -11.1658\n",
      "Epoch [3500/6000] | Loss D: -16.7770 | Loss G: -70.8150 | Score: 0.5074 | Critic Loss: -18.2137\n",
      "Epoch [3510/6000] | Loss D: -16.4656 | Loss G: -89.8306 | Score: 0.4979 | Critic Loss: -17.5042\n",
      "Epoch [3520/6000] | Loss D: -5.4106 | Loss G: -93.9058 | Score: 0.4949 | Critic Loss: -6.3328\n",
      "Epoch [3530/6000] | Loss D: -17.9485 | Loss G: -86.8049 | Score: 0.4957 | Critic Loss: -19.1264\n",
      "Epoch [3540/6000] | Loss D: -9.8379 | Loss G: -69.8968 | Score: 0.5000 | Critic Loss: -11.5494\n",
      "Epoch [3550/6000] | Loss D: -13.3428 | Loss G: -86.9878 | Score: 0.5013 | Critic Loss: -14.1813\n",
      "Epoch [3560/6000] | Loss D: -6.7151 | Loss G: -95.9056 | Score: 0.4889 | Critic Loss: -7.9482\n",
      "Epoch [3570/6000] | Loss D: -13.8129 | Loss G: -77.1352 | Score: 0.4970 | Critic Loss: -15.2498\n",
      "Epoch [3580/6000] | Loss D: -11.7702 | Loss G: -79.9119 | Score: 0.4861 | Critic Loss: -13.2279\n",
      "Epoch [3590/6000] | Loss D: -7.9416 | Loss G: -106.0995 | Score: 0.4998 | Critic Loss: -8.8818\n",
      "Epoch [3600/6000] | Loss D: -10.4387 | Loss G: -88.5760 | Score: 0.5052 | Critic Loss: -11.8697\n",
      "Epoch [3610/6000] | Loss D: -18.6750 | Loss G: -71.8268 | Score: 0.4954 | Critic Loss: -19.7023\n",
      "Epoch [3620/6000] | Loss D: -10.5006 | Loss G: -90.7365 | Score: 0.4951 | Critic Loss: -11.4227\n",
      "Epoch [3630/6000] | Loss D: -10.8062 | Loss G: -88.9682 | Score: 0.4904 | Critic Loss: -12.5738\n",
      "Epoch [3640/6000] | Loss D: -15.7109 | Loss G: -83.7098 | Score: 0.5105 | Critic Loss: -16.5679\n",
      "Epoch [3650/6000] | Loss D: -13.8559 | Loss G: -70.7958 | Score: 0.5020 | Critic Loss: -15.2931\n",
      "Epoch [3660/6000] | Loss D: -14.2260 | Loss G: -84.0430 | Score: 0.5034 | Critic Loss: -15.5172\n",
      "Epoch [3670/6000] | Loss D: -18.2942 | Loss G: -91.0063 | Score: 0.4949 | Critic Loss: -19.5925\n",
      "Epoch [3680/6000] | Loss D: -7.8939 | Loss G: -78.9135 | Score: 0.5025 | Critic Loss: -8.6359\n",
      "Epoch [3690/6000] | Loss D: -10.4947 | Loss G: -93.3666 | Score: 0.5042 | Critic Loss: -11.8987\n",
      "Epoch [3700/6000] | Loss D: -14.0799 | Loss G: -79.9530 | Score: 0.4991 | Critic Loss: -15.5133\n",
      "Epoch [3710/6000] | Loss D: -16.9843 | Loss G: -101.7865 | Score: 0.5015 | Critic Loss: -18.2786\n",
      "Epoch [3720/6000] | Loss D: -12.8953 | Loss G: -83.3383 | Score: 0.5042 | Critic Loss: -14.4235\n",
      "Epoch [3730/6000] | Loss D: -14.3156 | Loss G: -92.9861 | Score: 0.5084 | Critic Loss: -15.2665\n",
      "Epoch [3740/6000] | Loss D: -11.1016 | Loss G: -72.0716 | Score: 0.4974 | Critic Loss: -12.5360\n",
      "Epoch [3750/6000] | Loss D: -7.8506 | Loss G: -107.1985 | Score: 0.4979 | Critic Loss: -8.7971\n",
      "Epoch [3760/6000] | Loss D: -11.0613 | Loss G: -94.8471 | Score: 0.4982 | Critic Loss: -12.2546\n",
      "Epoch [3770/6000] | Loss D: -12.6101 | Loss G: -78.7712 | Score: 0.5081 | Critic Loss: -13.8234\n",
      "Epoch [3780/6000] | Loss D: -14.9726 | Loss G: -95.6094 | Score: 0.5104 | Critic Loss: -15.7659\n",
      "Epoch [3790/6000] | Loss D: -9.4059 | Loss G: -89.2083 | Score: 0.4932 | Critic Loss: -10.5177\n",
      "Epoch [3800/6000] | Loss D: -11.3454 | Loss G: -90.8884 | Score: 0.4910 | Critic Loss: -12.2900\n",
      "Epoch [3810/6000] | Loss D: -13.2717 | Loss G: -93.5355 | Score: 0.5064 | Critic Loss: -14.5995\n",
      "Epoch [3820/6000] | Loss D: -2.8929 | Loss G: -79.8746 | Score: 0.4929 | Critic Loss: -3.9325\n",
      "Epoch [3830/6000] | Loss D: -15.1994 | Loss G: -105.3143 | Score: 0.4935 | Critic Loss: -16.5467\n",
      "Epoch [3840/6000] | Loss D: -15.3763 | Loss G: -79.7992 | Score: 0.4849 | Critic Loss: -16.4359\n",
      "Epoch [3850/6000] | Loss D: -10.3908 | Loss G: -107.6452 | Score: 0.4938 | Critic Loss: -11.6296\n",
      "Epoch [3860/6000] | Loss D: -0.5043 | Loss G: -97.4634 | Score: 0.5042 | Critic Loss: -1.6370\n",
      "Epoch [3870/6000] | Loss D: -11.2111 | Loss G: -84.4405 | Score: 0.4915 | Critic Loss: -12.9961\n",
      "Epoch [3880/6000] | Loss D: -12.4859 | Loss G: -102.7588 | Score: 0.4930 | Critic Loss: -14.1641\n",
      "Epoch [3890/6000] | Loss D: -13.3392 | Loss G: -97.2084 | Score: 0.4926 | Critic Loss: -15.1050\n",
      "Epoch [3900/6000] | Loss D: -10.5158 | Loss G: -80.6144 | Score: 0.5009 | Critic Loss: -11.6411\n",
      "Epoch [3910/6000] | Loss D: -8.8329 | Loss G: -92.0180 | Score: 0.4968 | Critic Loss: -10.6003\n",
      "Epoch [3920/6000] | Loss D: -8.6755 | Loss G: -98.5278 | Score: 0.4912 | Critic Loss: -9.8585\n",
      "Epoch [3930/6000] | Loss D: -15.3614 | Loss G: -89.4298 | Score: 0.5008 | Critic Loss: -16.6865\n",
      "Epoch [3940/6000] | Loss D: -1.3376 | Loss G: -83.6139 | Score: 0.5015 | Critic Loss: -2.8639\n",
      "Epoch [3950/6000] | Loss D: -8.8586 | Loss G: -85.9449 | Score: 0.4982 | Critic Loss: -10.3053\n",
      "Epoch [3960/6000] | Loss D: -12.5412 | Loss G: -105.4045 | Score: 0.4987 | Critic Loss: -13.8488\n",
      "Epoch [3970/6000] | Loss D: -14.9797 | Loss G: -96.1138 | Score: 0.5043 | Critic Loss: -16.0510\n",
      "Epoch [3980/6000] | Loss D: -14.0591 | Loss G: -76.8013 | Score: 0.4907 | Critic Loss: -15.0772\n",
      "Epoch [3990/6000] | Loss D: -8.2716 | Loss G: -78.1953 | Score: 0.4955 | Critic Loss: -9.5142\n",
      "Epoch [4000/6000] | Loss D: -8.9726 | Loss G: -101.4113 | Score: 0.4945 | Critic Loss: -10.1809\n",
      "Epoch [4010/6000] | Loss D: -10.2717 | Loss G: -74.3557 | Score: 0.4938 | Critic Loss: -11.5491\n",
      "Epoch [4020/6000] | Loss D: -9.2462 | Loss G: -91.5455 | Score: 0.4956 | Critic Loss: -10.9222\n",
      "Epoch [4030/6000] | Loss D: -9.1666 | Loss G: -97.2569 | Score: 0.4933 | Critic Loss: -10.4603\n",
      "Epoch [4040/6000] | Loss D: -19.1580 | Loss G: -89.1267 | Score: 0.4954 | Critic Loss: -20.2012\n",
      "Epoch [4050/6000] | Loss D: -17.8065 | Loss G: -102.8395 | Score: 0.4934 | Critic Loss: -18.9212\n",
      "Epoch [4060/6000] | Loss D: -13.3521 | Loss G: -85.6229 | Score: 0.4974 | Critic Loss: -14.4132\n",
      "Epoch [4070/6000] | Loss D: -18.2712 | Loss G: -85.7510 | Score: 0.4940 | Critic Loss: -19.4492\n",
      "Epoch [4080/6000] | Loss D: -8.9688 | Loss G: -93.5890 | Score: 0.4973 | Critic Loss: -10.2648\n",
      "Epoch [4090/6000] | Loss D: -17.3267 | Loss G: -100.9937 | Score: 0.5046 | Critic Loss: -18.2772\n",
      "Epoch [4100/6000] | Loss D: -7.5499 | Loss G: -94.2164 | Score: 0.4933 | Critic Loss: -8.6401\n",
      "Epoch [4110/6000] | Loss D: -18.0082 | Loss G: -91.3331 | Score: 0.4939 | Critic Loss: -19.2546\n",
      "Epoch [4120/6000] | Loss D: -0.4889 | Loss G: -101.6125 | Score: 0.4868 | Critic Loss: -1.6932\n",
      "Epoch [4130/6000] | Loss D: -12.3636 | Loss G: -70.4158 | Score: 0.5013 | Critic Loss: -13.7367\n",
      "Epoch [4140/6000] | Loss D: -4.5071 | Loss G: -95.7773 | Score: 0.5013 | Critic Loss: -6.0195\n",
      "Epoch [4150/6000] | Loss D: -12.3868 | Loss G: -96.5125 | Score: 0.4921 | Critic Loss: -14.1530\n",
      "Epoch [4160/6000] | Loss D: -12.1107 | Loss G: -103.8694 | Score: 0.4821 | Critic Loss: -13.0322\n",
      "Epoch [4170/6000] | Loss D: -8.5449 | Loss G: -82.5300 | Score: 0.4945 | Critic Loss: -9.8789\n",
      "Epoch [4180/6000] | Loss D: -4.3354 | Loss G: -101.9080 | Score: 0.4918 | Critic Loss: -5.3127\n",
      "Epoch [4190/6000] | Loss D: -11.5195 | Loss G: -89.1912 | Score: 0.4917 | Critic Loss: -12.8358\n",
      "Epoch [4200/6000] | Loss D: -12.2836 | Loss G: -89.6021 | Score: 0.4954 | Critic Loss: -13.1887\n",
      "Epoch [4210/6000] | Loss D: -5.3054 | Loss G: -94.0504 | Score: 0.4987 | Critic Loss: -6.4694\n",
      "Epoch [4220/6000] | Loss D: -8.6315 | Loss G: -92.9521 | Score: 0.4886 | Critic Loss: -9.8641\n",
      "Epoch [4230/6000] | Loss D: -8.1476 | Loss G: -109.1659 | Score: 0.4899 | Critic Loss: -9.3225\n",
      "Epoch [4240/6000] | Loss D: -9.9251 | Loss G: -103.5461 | Score: 0.4932 | Critic Loss: -11.4753\n",
      "Epoch [4250/6000] | Loss D: -8.9321 | Loss G: -96.6667 | Score: 0.4916 | Critic Loss: -9.8899\n",
      "Epoch [4260/6000] | Loss D: -8.8631 | Loss G: -95.7755 | Score: 0.4865 | Critic Loss: -10.2568\n",
      "Epoch [4270/6000] | Loss D: -4.0105 | Loss G: -92.4198 | Score: 0.4964 | Critic Loss: -5.2506\n",
      "Epoch [4280/6000] | Loss D: -20.4592 | Loss G: -105.3491 | Score: 0.4835 | Critic Loss: -21.6385\n",
      "Epoch [4290/6000] | Loss D: -9.3096 | Loss G: -83.4402 | Score: 0.4925 | Critic Loss: -11.1801\n",
      "Epoch [4300/6000] | Loss D: -13.6039 | Loss G: -95.5312 | Score: 0.4877 | Critic Loss: -14.5341\n",
      "Epoch [4310/6000] | Loss D: -6.4867 | Loss G: -103.4491 | Score: 0.4963 | Critic Loss: -8.1649\n",
      "Epoch [4320/6000] | Loss D: -13.3176 | Loss G: -103.9779 | Score: 0.4946 | Critic Loss: -14.7323\n",
      "Epoch [4330/6000] | Loss D: -5.2084 | Loss G: -90.6364 | Score: 0.4810 | Critic Loss: -6.4986\n",
      "Epoch [4340/6000] | Loss D: -5.1897 | Loss G: -113.3373 | Score: 0.5073 | Critic Loss: -6.7963\n",
      "Epoch [4350/6000] | Loss D: -13.1823 | Loss G: -86.0302 | Score: 0.4841 | Critic Loss: -14.5442\n",
      "Epoch [4360/6000] | Loss D: -11.9087 | Loss G: -119.4034 | Score: 0.4931 | Critic Loss: -12.8589\n",
      "Epoch [4370/6000] | Loss D: -14.0333 | Loss G: -103.1137 | Score: 0.4894 | Critic Loss: -15.4474\n",
      "Epoch [4380/6000] | Loss D: -11.6341 | Loss G: -104.2481 | Score: 0.4921 | Critic Loss: -12.7464\n",
      "Epoch [4390/6000] | Loss D: -10.3306 | Loss G: -88.6701 | Score: 0.4875 | Critic Loss: -12.1416\n",
      "Epoch [4400/6000] | Loss D: -11.2801 | Loss G: -101.1223 | Score: 0.4892 | Critic Loss: -12.3601\n",
      "Epoch [4410/6000] | Loss D: -4.2706 | Loss G: -98.9700 | Score: 0.5011 | Critic Loss: -5.5245\n",
      "Epoch [4420/6000] | Loss D: -11.9647 | Loss G: -108.8905 | Score: 0.4965 | Critic Loss: -13.4497\n",
      "Epoch [4430/6000] | Loss D: -12.4828 | Loss G: -86.2943 | Score: 0.4907 | Critic Loss: -14.1351\n",
      "Epoch [4440/6000] | Loss D: -15.0885 | Loss G: -89.2387 | Score: 0.4874 | Critic Loss: -16.2416\n",
      "Epoch [4450/6000] | Loss D: -19.5253 | Loss G: -82.9298 | Score: 0.4993 | Critic Loss: -20.6039\n",
      "Epoch [4460/6000] | Loss D: -10.3508 | Loss G: -107.7230 | Score: 0.4995 | Critic Loss: -11.6486\n",
      "Epoch [4470/6000] | Loss D: -11.3070 | Loss G: -93.4082 | Score: 0.4935 | Critic Loss: -12.4808\n",
      "Epoch [4480/6000] | Loss D: -19.0771 | Loss G: -91.6170 | Score: 0.5002 | Critic Loss: -20.6503\n",
      "Epoch [4490/6000] | Loss D: -10.7901 | Loss G: -99.4775 | Score: 0.5019 | Critic Loss: -12.0456\n",
      "Epoch [4500/6000] | Loss D: -14.1228 | Loss G: -99.6440 | Score: 0.5013 | Critic Loss: -15.6137\n",
      "Epoch [4510/6000] | Loss D: -12.1411 | Loss G: -99.3305 | Score: 0.4889 | Critic Loss: -13.2601\n",
      "Epoch [4520/6000] | Loss D: -12.7442 | Loss G: -105.1899 | Score: 0.4914 | Critic Loss: -14.4581\n",
      "Epoch [4530/6000] | Loss D: -8.7283 | Loss G: -108.4300 | Score: 0.5075 | Critic Loss: -10.2924\n",
      "Epoch [4540/6000] | Loss D: -4.1367 | Loss G: -109.0272 | Score: 0.4929 | Critic Loss: -5.4293\n",
      "Epoch [4550/6000] | Loss D: -5.8705 | Loss G: -94.0744 | Score: 0.5005 | Critic Loss: -7.5345\n",
      "Epoch [4560/6000] | Loss D: -8.8504 | Loss G: -104.9922 | Score: 0.4946 | Critic Loss: -9.8106\n",
      "Epoch [4570/6000] | Loss D: -7.2591 | Loss G: -104.9923 | Score: 0.4950 | Critic Loss: -8.9238\n",
      "Epoch [4580/6000] | Loss D: -13.0998 | Loss G: -95.3921 | Score: 0.5008 | Critic Loss: -14.5279\n",
      "Epoch [4590/6000] | Loss D: -19.1632 | Loss G: -104.3986 | Score: 0.4934 | Critic Loss: -20.8088\n",
      "Epoch [4600/6000] | Loss D: -7.1612 | Loss G: -104.4199 | Score: 0.4909 | Critic Loss: -8.7415\n",
      "Epoch [4610/6000] | Loss D: -9.5451 | Loss G: -105.6838 | Score: 0.4915 | Critic Loss: -10.9124\n",
      "Epoch [4620/6000] | Loss D: -12.1164 | Loss G: -81.0512 | Score: 0.4903 | Critic Loss: -13.8624\n",
      "Epoch [4630/6000] | Loss D: -6.6597 | Loss G: -102.7196 | Score: 0.4906 | Critic Loss: -7.6573\n",
      "Epoch [4640/6000] | Loss D: -7.1640 | Loss G: -98.1842 | Score: 0.4998 | Critic Loss: -8.6854\n",
      "Epoch [4650/6000] | Loss D: -4.7939 | Loss G: -104.2744 | Score: 0.4929 | Critic Loss: -6.4444\n",
      "Epoch [4660/6000] | Loss D: -13.0730 | Loss G: -86.9258 | Score: 0.4890 | Critic Loss: -14.6876\n",
      "Epoch [4670/6000] | Loss D: -15.0420 | Loss G: -109.0462 | Score: 0.5016 | Critic Loss: -16.8896\n",
      "Epoch [4680/6000] | Loss D: -18.5553 | Loss G: -115.9518 | Score: 0.4946 | Critic Loss: -19.5265\n",
      "Epoch [4690/6000] | Loss D: -5.0176 | Loss G: -86.7538 | Score: 0.4977 | Critic Loss: -6.8404\n",
      "Epoch [4700/6000] | Loss D: -5.0060 | Loss G: -101.0528 | Score: 0.4944 | Critic Loss: -6.5044\n",
      "Epoch [4710/6000] | Loss D: -12.1825 | Loss G: -116.2897 | Score: 0.4943 | Critic Loss: -13.3324\n",
      "Epoch [4720/6000] | Loss D: -7.8664 | Loss G: -101.8189 | Score: 0.4964 | Critic Loss: -9.3065\n",
      "Epoch [4730/6000] | Loss D: -11.6234 | Loss G: -107.4403 | Score: 0.5126 | Critic Loss: -13.4464\n",
      "Epoch [4740/6000] | Loss D: -15.1118 | Loss G: -94.6550 | Score: 0.4905 | Critic Loss: -16.3899\n",
      "Epoch [4750/6000] | Loss D: -8.2325 | Loss G: -102.6327 | Score: 0.4876 | Critic Loss: -10.0241\n",
      "Epoch [4760/6000] | Loss D: -9.9520 | Loss G: -114.2157 | Score: 0.5032 | Critic Loss: -11.4753\n",
      "Epoch [4770/6000] | Loss D: -9.6702 | Loss G: -96.1174 | Score: 0.4964 | Critic Loss: -11.1911\n",
      "Epoch [4780/6000] | Loss D: -17.2893 | Loss G: -109.3755 | Score: 0.5017 | Critic Loss: -18.7542\n",
      "Epoch [4790/6000] | Loss D: -7.8662 | Loss G: -106.6368 | Score: 0.5096 | Critic Loss: -9.3978\n",
      "Epoch [4800/6000] | Loss D: -10.0528 | Loss G: -118.2307 | Score: 0.5014 | Critic Loss: -11.4233\n",
      "Epoch [4810/6000] | Loss D: -12.2039 | Loss G: -93.5988 | Score: 0.4814 | Critic Loss: -13.8833\n",
      "Epoch [4820/6000] | Loss D: -12.2133 | Loss G: -111.2552 | Score: 0.4976 | Critic Loss: -13.0054\n",
      "Epoch [4830/6000] | Loss D: -5.5901 | Loss G: -105.0813 | Score: 0.5007 | Critic Loss: -7.0863\n",
      "Epoch [4840/6000] | Loss D: -3.7594 | Loss G: -112.6023 | Score: 0.4939 | Critic Loss: -4.9492\n",
      "Epoch [4850/6000] | Loss D: -13.8058 | Loss G: -91.2347 | Score: 0.4909 | Critic Loss: -15.4467\n",
      "Epoch [4860/6000] | Loss D: -16.0085 | Loss G: -105.2711 | Score: 0.4973 | Critic Loss: -17.3717\n",
      "Epoch [4870/6000] | Loss D: -8.5660 | Loss G: -122.4201 | Score: 0.4992 | Critic Loss: -9.5174\n",
      "Epoch [4880/6000] | Loss D: -8.1887 | Loss G: -103.5781 | Score: 0.4981 | Critic Loss: -10.0693\n",
      "Epoch [4890/6000] | Loss D: -7.1877 | Loss G: -109.5308 | Score: 0.4785 | Critic Loss: -8.2122\n",
      "Epoch [4900/6000] | Loss D: -16.6566 | Loss G: -101.4716 | Score: 0.4856 | Critic Loss: -18.3401\n",
      "Epoch [4910/6000] | Loss D: -12.1802 | Loss G: -108.5453 | Score: 0.4859 | Critic Loss: -13.1089\n",
      "Epoch [4920/6000] | Loss D: -3.6361 | Loss G: -101.4601 | Score: 0.4917 | Critic Loss: -5.2816\n",
      "Epoch [4930/6000] | Loss D: -12.0188 | Loss G: -99.7150 | Score: 0.4979 | Critic Loss: -13.4987\n",
      "Epoch [4940/6000] | Loss D: -7.6908 | Loss G: -110.2707 | Score: 0.4840 | Critic Loss: -8.8086\n",
      "Epoch [4950/6000] | Loss D: -10.3291 | Loss G: -102.6260 | Score: 0.4940 | Critic Loss: -11.5526\n",
      "Epoch [4960/6000] | Loss D: -22.3320 | Loss G: -116.9784 | Score: 0.4922 | Critic Loss: -23.7044\n",
      "Epoch [4970/6000] | Loss D: -13.1550 | Loss G: -102.1261 | Score: 0.4885 | Critic Loss: -14.6313\n",
      "Epoch [4980/6000] | Loss D: -13.4012 | Loss G: -98.3686 | Score: 0.4953 | Critic Loss: -14.3998\n",
      "Epoch [4990/6000] | Loss D: -11.7817 | Loss G: -103.8092 | Score: 0.5007 | Critic Loss: -12.9103\n",
      "Epoch [5000/6000] | Loss D: -19.6330 | Loss G: -95.1830 | Score: 0.4953 | Critic Loss: -20.9322\n",
      "Epoch [5010/6000] | Loss D: -12.4122 | Loss G: -100.4878 | Score: 0.4907 | Critic Loss: -13.8788\n",
      "Epoch [5020/6000] | Loss D: -12.2128 | Loss G: -99.2467 | Score: 0.4883 | Critic Loss: -13.3658\n",
      "Epoch [5030/6000] | Loss D: -19.2142 | Loss G: -102.8172 | Score: 0.5061 | Critic Loss: -20.8553\n",
      "Epoch [5040/6000] | Loss D: -15.0993 | Loss G: -124.4967 | Score: 0.4915 | Critic Loss: -16.5280\n",
      "Epoch [5050/6000] | Loss D: -12.7671 | Loss G: -104.0188 | Score: 0.4906 | Critic Loss: -14.7433\n",
      "Epoch [5060/6000] | Loss D: -8.2701 | Loss G: -103.2778 | Score: 0.4928 | Critic Loss: -9.6069\n",
      "Epoch [5070/6000] | Loss D: -12.0299 | Loss G: -106.3111 | Score: 0.4907 | Critic Loss: -13.8679\n",
      "Epoch [5080/6000] | Loss D: -9.9769 | Loss G: -99.8062 | Score: 0.5067 | Critic Loss: -11.7363\n",
      "Epoch [5090/6000] | Loss D: -9.2605 | Loss G: -104.9854 | Score: 0.4882 | Critic Loss: -10.8702\n",
      "Epoch [5100/6000] | Loss D: -11.8012 | Loss G: -113.1650 | Score: 0.4837 | Critic Loss: -12.8823\n",
      "Epoch [5110/6000] | Loss D: -18.1378 | Loss G: -99.5099 | Score: 0.4930 | Critic Loss: -19.6713\n",
      "Epoch [5120/6000] | Loss D: -13.0770 | Loss G: -115.5228 | Score: 0.4823 | Critic Loss: -14.4775\n",
      "Epoch [5130/6000] | Loss D: -16.2259 | Loss G: -88.7495 | Score: 0.4949 | Critic Loss: -17.6039\n",
      "Epoch [5140/6000] | Loss D: -14.3154 | Loss G: -120.1182 | Score: 0.4926 | Critic Loss: -15.3687\n",
      "Epoch [5150/6000] | Loss D: -6.0621 | Loss G: -107.8294 | Score: 0.4961 | Critic Loss: -7.4992\n",
      "Epoch [5160/6000] | Loss D: -10.5439 | Loss G: -109.5753 | Score: 0.4937 | Critic Loss: -11.7887\n",
      "Epoch [5170/6000] | Loss D: -13.7332 | Loss G: -102.7301 | Score: 0.4888 | Critic Loss: -15.3593\n",
      "Epoch [5180/6000] | Loss D: -2.2631 | Loss G: -118.1891 | Score: 0.4885 | Critic Loss: -3.4294\n",
      "Epoch [5190/6000] | Loss D: -12.2823 | Loss G: -97.1346 | Score: 0.4938 | Critic Loss: -14.0519\n",
      "Epoch [5200/6000] | Loss D: -5.5334 | Loss G: -117.3196 | Score: 0.4909 | Critic Loss: -6.7131\n",
      "Epoch [5210/6000] | Loss D: -12.8773 | Loss G: -113.5337 | Score: 0.4846 | Critic Loss: -14.4848\n",
      "Epoch [5220/6000] | Loss D: -11.2257 | Loss G: -110.2455 | Score: 0.4858 | Critic Loss: -12.3890\n",
      "Epoch [5230/6000] | Loss D: -10.9077 | Loss G: -114.0229 | Score: 0.4904 | Critic Loss: -12.5307\n",
      "Epoch [5240/6000] | Loss D: -7.5959 | Loss G: -94.4507 | Score: 0.4976 | Critic Loss: -9.2984\n",
      "Epoch [5250/6000] | Loss D: -16.2932 | Loss G: -98.1493 | Score: 0.4974 | Critic Loss: -17.3376\n",
      "Epoch [5260/6000] | Loss D: -9.7484 | Loss G: -109.6770 | Score: 0.4953 | Critic Loss: -12.1270\n",
      "Epoch [5270/6000] | Loss D: -11.5118 | Loss G: -105.6468 | Score: 0.4949 | Critic Loss: -12.8824\n",
      "Epoch [5280/6000] | Loss D: -20.0274 | Loss G: -105.2012 | Score: 0.4949 | Critic Loss: -21.8247\n",
      "Epoch [5290/6000] | Loss D: -10.1848 | Loss G: -79.5289 | Score: 0.4801 | Critic Loss: -12.2389\n",
      "Epoch [5300/6000] | Loss D: -15.8947 | Loss G: -115.1665 | Score: 0.4978 | Critic Loss: -16.9236\n",
      "Epoch [5310/6000] | Loss D: -5.0522 | Loss G: -107.7237 | Score: 0.4937 | Critic Loss: -6.7267\n",
      "Epoch [5320/6000] | Loss D: -7.4411 | Loss G: -109.1114 | Score: 0.4959 | Critic Loss: -8.4593\n",
      "Epoch [5330/6000] | Loss D: -11.1318 | Loss G: -116.1420 | Score: 0.4923 | Critic Loss: -12.7984\n",
      "Epoch [5340/6000] | Loss D: -12.3505 | Loss G: -103.1762 | Score: 0.4919 | Critic Loss: -13.8798\n",
      "Epoch [5350/6000] | Loss D: -13.7773 | Loss G: -114.6899 | Score: 0.4926 | Critic Loss: -14.9652\n",
      "Epoch [5360/6000] | Loss D: -15.8467 | Loss G: -102.7091 | Score: 0.4842 | Critic Loss: -18.0551\n",
      "Epoch [5370/6000] | Loss D: -16.0150 | Loss G: -110.9296 | Score: 0.4964 | Critic Loss: -17.3837\n",
      "Epoch [5380/6000] | Loss D: -12.5587 | Loss G: -117.1266 | Score: 0.4958 | Critic Loss: -13.4125\n",
      "Epoch [5390/6000] | Loss D: -15.4110 | Loss G: -114.7429 | Score: 0.4966 | Critic Loss: -16.6366\n",
      "Epoch [5400/6000] | Loss D: -11.0904 | Loss G: -107.6559 | Score: 0.5030 | Critic Loss: -12.5639\n",
      "Epoch [5410/6000] | Loss D: -10.0894 | Loss G: -100.1231 | Score: 0.4896 | Critic Loss: -11.9267\n",
      "Epoch [5420/6000] | Loss D: -6.8815 | Loss G: -125.7382 | Score: 0.5073 | Critic Loss: -7.9501\n",
      "Epoch [5430/6000] | Loss D: -13.2053 | Loss G: -100.8424 | Score: 0.4921 | Critic Loss: -15.1536\n",
      "Epoch [5440/6000] | Loss D: -18.8491 | Loss G: -107.8902 | Score: 0.4879 | Critic Loss: -19.8008\n",
      "Epoch [5450/6000] | Loss D: -16.7593 | Loss G: -108.8871 | Score: 0.4961 | Critic Loss: -18.8925\n",
      "Epoch [5460/6000] | Loss D: -10.4112 | Loss G: -125.3414 | Score: 0.4899 | Critic Loss: -11.3946\n",
      "Epoch [5470/6000] | Loss D: -18.2076 | Loss G: -110.5124 | Score: 0.5019 | Critic Loss: -20.5259\n",
      "Epoch [5480/6000] | Loss D: -13.9675 | Loss G: -112.9753 | Score: 0.4870 | Critic Loss: -15.5886\n",
      "Epoch [5490/6000] | Loss D: -13.3188 | Loss G: -120.0256 | Score: 0.4887 | Critic Loss: -15.0619\n",
      "Epoch [5500/6000] | Loss D: -20.5331 | Loss G: -114.4879 | Score: 0.4994 | Critic Loss: -22.1535\n",
      "Epoch [5510/6000] | Loss D: -15.4307 | Loss G: -96.1212 | Score: 0.4876 | Critic Loss: -17.2435\n",
      "Epoch [5520/6000] | Loss D: -11.2587 | Loss G: -122.4689 | Score: 0.4991 | Critic Loss: -12.8217\n",
      "Epoch [5530/6000] | Loss D: -14.0718 | Loss G: -109.6019 | Score: 0.4948 | Critic Loss: -15.2779\n",
      "Epoch [5540/6000] | Loss D: -6.8269 | Loss G: -124.4355 | Score: 0.4967 | Critic Loss: -8.1867\n",
      "Epoch [5550/6000] | Loss D: -15.7667 | Loss G: -111.9623 | Score: 0.4813 | Critic Loss: -17.1712\n",
      "Epoch [5560/6000] | Loss D: -15.6794 | Loss G: -109.5750 | Score: 0.4933 | Critic Loss: -17.4514\n",
      "Epoch [5570/6000] | Loss D: -6.9517 | Loss G: -110.7887 | Score: 0.4943 | Critic Loss: -7.9507\n",
      "Epoch [5580/6000] | Loss D: -10.8033 | Loss G: -118.2490 | Score: 0.4938 | Critic Loss: -12.0418\n",
      "Epoch [5590/6000] | Loss D: -11.9448 | Loss G: -104.2671 | Score: 0.4991 | Critic Loss: -13.2927\n",
      "Epoch [5600/6000] | Loss D: -11.0715 | Loss G: -113.7010 | Score: 0.4903 | Critic Loss: -12.8147\n",
      "Epoch [5610/6000] | Loss D: -17.0422 | Loss G: -115.1846 | Score: 0.5072 | Critic Loss: -18.6854\n",
      "Epoch [5620/6000] | Loss D: -15.1812 | Loss G: -117.5704 | Score: 0.4951 | Critic Loss: -16.2242\n",
      "Epoch [5630/6000] | Loss D: -8.3126 | Loss G: -84.9347 | Score: 0.4952 | Critic Loss: -10.1985\n",
      "Epoch [5640/6000] | Loss D: -9.3739 | Loss G: -122.9785 | Score: 0.4915 | Critic Loss: -11.0643\n",
      "Epoch [5650/6000] | Loss D: -14.7857 | Loss G: -111.7925 | Score: 0.4915 | Critic Loss: -16.7388\n",
      "Epoch [5660/6000] | Loss D: -15.2798 | Loss G: -123.7355 | Score: 0.5024 | Critic Loss: -16.3197\n",
      "Epoch [5670/6000] | Loss D: -14.6780 | Loss G: -95.1441 | Score: 0.4914 | Critic Loss: -16.5893\n",
      "Epoch [5680/6000] | Loss D: -11.5110 | Loss G: -110.1692 | Score: 0.4971 | Critic Loss: -13.3900\n",
      "Epoch [5690/6000] | Loss D: -15.0243 | Loss G: -127.3292 | Score: 0.4912 | Critic Loss: -16.6212\n",
      "Epoch [5700/6000] | Loss D: -14.8818 | Loss G: -107.1043 | Score: 0.4884 | Critic Loss: -16.2856\n",
      "Epoch [5710/6000] | Loss D: -7.7083 | Loss G: -110.6800 | Score: 0.4947 | Critic Loss: -9.1547\n",
      "Epoch [5720/6000] | Loss D: -15.7829 | Loss G: -115.1738 | Score: 0.4855 | Critic Loss: -17.0503\n",
      "Epoch [5730/6000] | Loss D: -4.5463 | Loss G: -115.8552 | Score: 0.4948 | Critic Loss: -6.0527\n",
      "Epoch [5740/6000] | Loss D: -13.9020 | Loss G: -105.0418 | Score: 0.4881 | Critic Loss: -14.9754\n",
      "Epoch [5750/6000] | Loss D: -8.1380 | Loss G: -111.9927 | Score: 0.4928 | Critic Loss: -9.2891\n",
      "Epoch [5760/6000] | Loss D: -18.1104 | Loss G: -95.0429 | Score: 0.4948 | Critic Loss: -19.1478\n",
      "Epoch [5770/6000] | Loss D: -13.8930 | Loss G: -125.3168 | Score: 0.4882 | Critic Loss: -15.2497\n",
      "Epoch [5780/6000] | Loss D: -5.5578 | Loss G: -109.1175 | Score: 0.4885 | Critic Loss: -7.0341\n",
      "Epoch [5790/6000] | Loss D: -12.4101 | Loss G: -103.1237 | Score: 0.4932 | Critic Loss: -13.9561\n",
      "Epoch [5800/6000] | Loss D: -1.9844 | Loss G: -134.1893 | Score: 0.4930 | Critic Loss: -3.0875\n",
      "Epoch [5810/6000] | Loss D: -9.2775 | Loss G: -113.8687 | Score: 0.4987 | Critic Loss: -10.7172\n",
      "Epoch [5820/6000] | Loss D: -12.9264 | Loss G: -102.2951 | Score: 0.5010 | Critic Loss: -14.6229\n",
      "Epoch [5830/6000] | Loss D: -18.4675 | Loss G: -107.1585 | Score: 0.4836 | Critic Loss: -20.4887\n",
      "Epoch [5840/6000] | Loss D: -15.7955 | Loss G: -120.5141 | Score: 0.4915 | Critic Loss: -17.2220\n",
      "Epoch [5850/6000] | Loss D: -7.8867 | Loss G: -121.8803 | Score: 0.4930 | Critic Loss: -9.5585\n",
      "Epoch [5860/6000] | Loss D: -9.1441 | Loss G: -98.6131 | Score: 0.4928 | Critic Loss: -10.6138\n",
      "Epoch [5870/6000] | Loss D: -19.0118 | Loss G: -111.9313 | Score: 0.4921 | Critic Loss: -20.4828\n",
      "Epoch [5880/6000] | Loss D: -14.5284 | Loss G: -124.5083 | Score: 0.4956 | Critic Loss: -15.7930\n",
      "Epoch [5890/6000] | Loss D: -12.3050 | Loss G: -116.1881 | Score: 0.4929 | Critic Loss: -13.5261\n",
      "Epoch [5900/6000] | Loss D: -15.0797 | Loss G: -103.0285 | Score: 0.4878 | Critic Loss: -16.6557\n",
      "Epoch [5910/6000] | Loss D: -17.1829 | Loss G: -114.3382 | Score: 0.4818 | Critic Loss: -18.5617\n",
      "Epoch [5920/6000] | Loss D: -6.7173 | Loss G: -119.1151 | Score: 0.4980 | Critic Loss: -7.9557\n",
      "Epoch [5930/6000] | Loss D: -14.9540 | Loss G: -109.7169 | Score: 0.4923 | Critic Loss: -16.7929\n",
      "Epoch [5940/6000] | Loss D: -20.1046 | Loss G: -115.3119 | Score: 0.4888 | Critic Loss: -21.6208\n",
      "Epoch [5950/6000] | Loss D: -18.4762 | Loss G: -120.0396 | Score: 0.4844 | Critic Loss: -20.2162\n",
      "Epoch [5960/6000] | Loss D: -7.0812 | Loss G: -107.2671 | Score: 0.4949 | Critic Loss: -8.8121\n",
      "Epoch [5970/6000] | Loss D: -8.9393 | Loss G: -122.0080 | Score: 0.4884 | Critic Loss: -10.3317\n",
      "Epoch [5980/6000] | Loss D: -20.8262 | Loss G: -117.9924 | Score: 0.4896 | Critic Loss: -22.4098\n",
      "Epoch [5990/6000] | Loss D: -10.8790 | Loss G: -102.4796 | Score: 0.4859 | Critic Loss: -12.2216\n",
      "Comb: (0.002, 0.0002, 10) | Lambda: 10 | Score: 0.4859\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComb: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Lambda: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombs[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m   save_models(generator, discriminator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdia_comb_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(combs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(combs[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lambda_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(combs[\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m---> 97\u001b[0m   \u001b[43msave_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdia_comb_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcombs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcombs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_lambda_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcombs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m   all_plots(losses_D, losses_G, scores, critic_losses)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m, in \u001b[0;36msave_tests\u001b[0;34m(generator, name_sufix)\u001b[0m\n\u001b[1;32m     11\u001b[0m fake_set \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;28mlen\u001b[39m(v_test),  latent_dim)), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#instead of batch_size should be len(v_test), but depends on your GPU power.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(t\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 30\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m---> 30\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m512\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_size)\n\u001b[1;32m     32\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_blocks(out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "# lr_combs = [(0.002, 0.0002, 5), (0.002, 0.0002, 10), (0.001, 0.0001, 10), (0.0002, 0.0002, 10)]\n",
    "lr_combs = [(0.002, 0.0002, 10)]\n",
    "\n",
    "lambda_gp = 10  # Coeficiente de penalización de gradiente\n",
    "n_critic = 5    # Número de pasos de entrenamiento del discriminador por cada paso del generador\n",
    "n_epochs = 6000\n",
    "\n",
    "# for combs[2] in [5,10,15]:\n",
    "for combs in lr_combs:\n",
    "\n",
    "  # Inicialización de listas para almacenar las pérdidas\n",
    "  scores = []\n",
    "  losses_G = []\n",
    "  losses_D = []\n",
    "  critic_losses = []\n",
    "  validity_real = []\n",
    "  validity_fake = []\n",
    "  generator = Generator()\n",
    "  discriminator = Discriminator()\n",
    "\n",
    "  if cuda:\n",
    "      generator.cuda()\n",
    "      discriminator.cuda()\n",
    "      # adversarial_loss.cuda()\n",
    "\n",
    "  # Initialize weights\n",
    "  generator.apply(weights_init_normal)\n",
    "  discriminator.apply(weights_init_normal)\n",
    "\n",
    "\n",
    "  # Optimizers\n",
    "  optimizer_G = torch.optim.Adam(generator.parameters(), lr=combs[0], betas=(b1, b2),weight_decay=1e-4)\n",
    "  optimizer_D = torch.optim.Adam(discriminator.parameters(), lr= combs[1], betas=( b1,  b2), weight_decay=1e-4)\n",
    "\n",
    "  Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "      for i, imgs in enumerate(dataloader):\n",
    "\n",
    "          # Configuración de tensores\n",
    "          real_imgs = imgs.to(device)\n",
    "          # real_imgs = Variable(imgs.type(Tensor))\n",
    "          optimizer_D.zero_grad()\n",
    "\n",
    "          # Generación de ruido y muestras falsas\n",
    "          z = torch.randn(imgs.size(0), 100).to(device)\n",
    "          fake_imgs = generator(z).detach()\n",
    "          fake_imgs.requires_grad = True\n",
    "\n",
    "          # Pérdida del discriminador\n",
    "          real_validity = discriminator(real_imgs)\n",
    "          fake_validity = discriminator(fake_imgs)\n",
    "          validity_real.append(real_validity.mean().item())\n",
    "          validity_fake.append(fake_validity.mean().item())\n",
    "          gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "          critic_loss = -torch.mean(real_validity) + torch.mean(fake_validity)\n",
    "          critic_losses.append(critic_loss.item())\n",
    "          loss_D = critic_loss + combs[2] * gradient_penalty\n",
    "\n",
    "          loss_D.backward()\n",
    "          optimizer_D.step()\n",
    "\n",
    "          # Entrenamiento del generador cada n_critic iteraciones\n",
    "          if i % n_critic == 0:\n",
    "              optimizer_G.zero_grad()\n",
    "\n",
    "              # Generación de muestras y cálculo de pérdida\n",
    "              gen_imgs = generator(z)\n",
    "              gen_validity = discriminator(gen_imgs)\n",
    "              loss_G = -torch.mean(gen_validity)\n",
    "\n",
    "              loss_G.backward()\n",
    "              optimizer_G.step()\n",
    "          \n",
    "          \n",
    "          # Almacenar las pérdidas al final de cada época\n",
    "          losses_G.append(loss_G.item())\n",
    "          losses_D.append(loss_D.item())\n",
    "\n",
    "      # # Mostrar las pérdidas cada 10 épocas\n",
    "      # if epoch % 10 == 0:\n",
    "      #     # print(f\"Epoch [{epoch}/{n_epochs}] | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f} | Score: {score:.4f}\")\n",
    "      #     print(f\"Epoch [{epoch}/{n_epochs}] | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f}\")\n",
    "      if epoch % 10 == 0:\n",
    "          score = calculate_score(generator, v_train, num_samples=100)\n",
    "          scores.append(score)\n",
    "          print(f\"Epoch [{epoch}/{n_epochs}] | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f} | Score: {score:.4f} | Critic Loss: {critic_loss.item():.4f}\")\n",
    "          # Guardar modelos\n",
    "          # torch.save(generator.state_dict(), f\"generator_epoch_{epoch}.pth\")\n",
    "          # torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch}.pth\")\n",
    "      \n",
    "\n",
    "  # score = calculate_score(generator, v_train, num_samples=100)\n",
    "  # scores.append(score)\n",
    "  print(f\"Comb: {combs} | Lambda: {combs[2]} | Score: {score:.4f}\")\n",
    "  save_models(generator, discriminator, \"dia_comb_\"+str(combs[0])+\"_\"+str(combs[1])+\"_lambda_\"+str(combs[2]))\n",
    "  save_tests(generator, \"dia_comb_\"+str(combs[0])+\"_\"+str(combs[1])+\"_lambda_\"+str(combs[2]))\n",
    "  all_plots(losses_D, losses_G, scores, critic_losses)\n",
    "  \n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7638, -2.4975, -1.4453,  ...,  1.5741, -0.1626, -2.1280],\n",
       "        [ 1.3258,  0.3816,  0.5806,  ...,  0.6039,  0.2208, -3.0766],\n",
       "        [ 0.8834,  0.2369,  1.1230,  ...,  0.1935,  2.2852, -0.3835],\n",
       "        ...,\n",
       "        [-1.2810, -0.8837, -0.5011,  ...,  0.7919,  0.7451,  1.5312],\n",
       "        [ 0.0097,  0.4265, -0.4662,  ..., -0.4942, -0.5325,  1.0509],\n",
       "        [ 1.7242, -0.3394, -0.3890,  ...,  0.8728, -0.7568,  1.2431]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(346, 1, 64, 64)\n",
      "len of fake set 346\n"
     ]
    }
   ],
   "source": [
    "fake_set = []\n",
    "t = torch.tensor(np.random.normal(0, 1, (len(v_test), latent_dim)), device='cuda').float()  # Ensure t is a float tensor\n",
    "t = generator(t).detach().cpu().numpy()\n",
    "print(t.shape)\n",
    "\n",
    "for i in range(0, t.shape[0]):\n",
    "  fake_set.append(np.rint(t[i][0]).astype(int))\n",
    "\n",
    "print(\"len of fake set\", len(fake_set))\n",
    "\n",
    "\n",
    "with open( \"./generations/fake_set_\"+\"dia_comb_\"+str(combs[0])+\"_\"+str(combs[1])+\"_lambda_\"+str(combs[2])+\".txt\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(fake_set, fp) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
